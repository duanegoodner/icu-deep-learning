{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Time Series Deep Learning and Adversarial Attacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. About this Notebook\n",
    "\n",
    "See the project [README](https://github.com/duanegoodner/lstm_adversarial_attack) for general information on the dataset and approach used in this notebook.\n",
    "\n",
    "The implementation details for this project are encapsulated in various classes and methods defined in modules under the project `src` directory, and various intermediate data structures and logs are saved in the project `data` directory. Most of the code in this notebook simply instantiates top-level classes and makes calls to their methods without revealing implementation details. Please look to code in the `src` and `data` directories if you interested in lower level details. The import paths as well as the terminal output shown in this notebook will provide some guidance on where to look within those directories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Project Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before running this notebook, review the project README at https://github.com/duanegoodner/lstm_adversarial_attack, and complete all steps in the \"How to run this project\" section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Imports\n",
    "Most of the necessary standard library imports and external package imports are handled modules in the `src` directory, but we need to import a few here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Standard Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 External Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Internal Project Modules and Sub-packages\n",
    "To help gain a sense of project structure, we will import internal packages and modules as-needed (i.e. immediately before the notebook code cells where they are first used). For now, we import the project `src` path defined in `lstm_adversarial_attack/notebooks/src_path`, add it to sys.path (so we can easily import project code), and we import project config files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import src_paths\n",
    "sys.path.append(str(src_paths.lstm_adversarial_attack_pkg))\n",
    "import lstm_adversarial_attack.config_paths as cfg_paths\n",
    "import lstm_adversarial_attack.config_settings as cfg_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Database Queries\n",
    "We need to run four queries on the MIMIC-III PostgreSQL database. The paths to files containing the queries are stored in a list as `DB_QUERIES` in the project `config_paths` file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PosixPath('/home/devspace/project/src/mimiciii_queries/icustay_detail.sql'),\n",
      " PosixPath('/home/devspace/project/src/mimiciii_queries/pivoted_bg.sql'),\n",
      " PosixPath('/home/devspace/project/src/mimiciii_queries/pivoted_lab.sql'),\n",
      " PosixPath('/home/devspace/project/src/mimiciii_queries/pivoted_vital.sql')]\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(cfg_paths.DB_QUERIES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To connect to the database, and execute the queries, we instantiate a `MimiciiiDatabaseAccess` object from module `mimiciii_database` of project sub-package `query_db` and use its .connect(), .run_sql_queries() and .close_connection() methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query 1 of 4\n",
      "Executing: /home/devspace/project/src/mimiciii_queries/icustay_detail.sql\n",
      "Done. Query time = 0.48 seconds\n",
      "Writing result to csv: /home/devspace/project/data/mimiciii_query_results/icustay_detail.csv\n",
      "Done. csv write time = 0.52 seconds\n",
      "\n",
      "Query 2 of 4\n",
      "Executing: /home/devspace/project/src/mimiciii_queries/pivoted_bg.sql\n",
      "Done. Query time = 16.95 seconds\n",
      "Writing result to csv: /home/devspace/project/data/mimiciii_query_results/pivoted_bg.csv\n",
      "Done. csv write time = 4.32 seconds\n",
      "\n",
      "Query 3 of 4\n",
      "Executing: /home/devspace/project/src/mimiciii_queries/pivoted_lab.sql\n",
      "Done. Query time = 24.69 seconds\n",
      "Writing result to csv: /home/devspace/project/data/mimiciii_query_results/pivoted_lab.csv\n",
      "Done. csv write time = 6.34 seconds\n",
      "\n",
      "Query 4 of 4\n",
      "Executing: /home/devspace/project/src/mimiciii_queries/pivoted_vital.sql\n",
      "Done. Query time = 63.48 seconds\n",
      "Writing result to csv: /home/devspace/project/data/mimiciii_query_results/pivoted_vital.csv\n",
      "Done. csv write time = 27.32 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import lstm_adversarial_attack.query_db.mimiciii_database as mdb\n",
    "\n",
    "db_access = mdb.MimiciiiDatabaseAccess(\n",
    "    dotenv_path=cfg_paths.DB_DOTENV_PATH, output_dir=cfg_paths.DB_OUTPUT_DIR\n",
    ")\n",
    "db_access.connect()\n",
    "db_query_results = db_access.run_sql_queries(\n",
    "    sql_query_paths=cfg_paths.DB_QUERIES\n",
    ")\n",
    "db_access.close_connection()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of each `.sql` query is saved to a `.csv` file. The path to each of these files is shown in the terminal output above. The output path of the queries is defined by variable `DB_OUTPUT_DIR` in the project `config_settings` file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Preprocessor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Instantiate a Preprocessor object\n",
    "We import the `preprocessor` module from internal sub-package `preprocess`, instantiate a `Preprocessor` object, and examine its .preprocess_modules data member. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import lstm_adversarial_attack.preprocess.preprocessor as pre\n",
    "preprocessor = pre.Preprocessor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get a general idea of how the `lstm_adversarial_attack.preprocess` sub-package works by looking at the `Preprocessor` object's `.preprocessor_modules` data member."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<class 'lstm_adversarial_attack.preprocess.prefilter.Prefilter'>,\n",
      " <class 'lstm_adversarial_attack.preprocess.icustay_measurement_combiner.ICUStayMeasurementCombiner'>,\n",
      " <class 'lstm_adversarial_attack.preprocess.sample_list_builder.FullAdmissionListBuilder'>,\n",
      " <class 'lstm_adversarial_attack.preprocess.feature_builder.FeatureBuilder'>,\n",
      " <class 'lstm_adversarial_attack.preprocess.feature_finalizer.FeatureFinalizer'>]\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint([item.__class__ for item in preprocessor.preprocess_modules])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Prefilter reads the database query outputs into Pandas Dataframes, removes all data related to patients younger than 18 years in age, ensures consistent column naming formats, and takes care of datatype details.\n",
    "* ICUStayMeasurementCombiner performs various joins (aka \"merges\" in the language of Pandas) to combine lab and vital sign measurement data with ICU stay data.\n",
    "* FullAdmissionListBuilder generates a list consisting of one FullAdmissionData object per ICU stay. The attributes of a FullAdmissionData object include ICU stay info, and a dataframe containing the measurement and timestamp data for all vital sign and lab data associated with the ICU stay.\n",
    "* FeatureBuilder resamples the time series datafame to one-hour intervals, imputes missing data, winsorizes measurement values (with cutoffs at the 5th and 95th global percentiles), and normalizes the measuremnt values so all data are between 0 and 1.\n",
    "* FeatureFinalizer selects the data observation time window (default starts at hospital admission time and ends 48 hours after admission). This module outputs the entire dataset features as a list of numpy arrays, and the mortality labels as a list of integers. These data structures (saved as .pickle files) will be convenient starting points when the `tune_train` and `attack` sub-packages need to create PyTorch Datasets.\n",
    "\n",
    "Now that we have a some background info, we are ready to run the Preprocessor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running preprocess module 1 of 5: Prefilter\n",
      "Incoming resources:\n",
      "/home/devspace/project/data/mimiciii_query_results/icustay_detail.csv\n",
      "/home/devspace/project/data/mimiciii_query_results/pivoted_bg.csv\n",
      "/home/devspace/project/data/mimiciii_query_results/pivoted_vital.csv\n",
      "/home/devspace/project/data/mimiciii_query_results/pivoted_lab.csv\n",
      "Done with Prefilter. Results saved to:\n",
      "path: /home/devspace/project/data/preprocess_checkpoints/1_prefilter/icustay.pickle, data_type: DataFrame\n",
      "path: /home/devspace/project/data/preprocess_checkpoints/1_prefilter/bg.pickle, data_type: DataFrame\n",
      "path: /home/devspace/project/data/preprocess_checkpoints/1_prefilter/vital.pickle, data_type: DataFrame\n",
      "path: /home/devspace/project/data/preprocess_checkpoints/1_prefilter/lab.pickle, data_type: DataFrame\n",
      "\n",
      "Running preprocess module 2 of 5: ICU Stay Data + Measurement Data Combiner\n",
      "Incoming resources:\n",
      "/home/devspace/project/data/preprocess_checkpoints/1_prefilter/icustay.pickle\n",
      "/home/devspace/project/data/preprocess_checkpoints/1_prefilter/bg.pickle\n",
      "/home/devspace/project/data/preprocess_checkpoints/1_prefilter/lab.pickle\n",
      "/home/devspace/project/data/preprocess_checkpoints/1_prefilter/vital.pickle\n",
      "Done with ICU Stay Data + Measurement Data Combiner. Results saved to:\n",
      "path: /home/devspace/project/data/preprocess_checkpoints/2_merged_stay_measurements/icustay_bg_lab_vital.pickle, data_type: DataFrame\n",
      "path: /home/devspace/project/data/preprocess_checkpoints/2_merged_stay_measurements/bg_lab_vital_summary_stats.pickle, data_type: DataFrame\n",
      "\n",
      "Running preprocess module 3 of 5: FullAdmission Object List Builder\n",
      "Incoming resources:\n",
      "/home/devspace/project/data/preprocess_checkpoints/2_merged_stay_measurements/icustay_bg_lab_vital.pickle\n",
      "Done with FullAdmission Object List Builder. Results saved to:\n",
      "path: /home/devspace/project/data/preprocess_checkpoints/3_full_admission_list/full_admission_list.pickle, data_type: list\n",
      "\n",
      "Running preprocess module 4 of 5: Feature Builder\n",
      "Incoming resources:\n",
      "/home/devspace/project/data/preprocess_checkpoints/3_full_admission_list/full_admission_list.pickle\n",
      "/home/devspace/project/data/preprocess_checkpoints/2_merged_stay_measurements/bg_lab_vital_summary_stats.pickle\n",
      "Done building features for sample 5000/41960\n",
      "Done building features for sample 10000/41960\n",
      "Done building features for sample 15000/41960\n",
      "Done building features for sample 20000/41960\n",
      "Done building features for sample 25000/41960\n",
      "Done building features for sample 30000/41960\n",
      "Done building features for sample 35000/41960\n",
      "Done building features for sample 40000/41960\n",
      "Done with Feature Builder. Results saved to:\n",
      "path: /home/devspace/project/data/preprocess_checkpoints/4_feature_builder/hadm_list_with_processed_dfs.pickle, data_type: list\n",
      "\n",
      "Running preprocess module 5 of 5: Feature Finalizer\n",
      "Incoming resources:\n",
      "/home/devspace/project/data/preprocess_checkpoints/4_feature_builder/hadm_list_with_processed_dfs.pickle\n",
      "Done with Feature Finalizer. Results saved to:\n",
      "path: /home/devspace/project/data/output_feature_finalizer/measurement_col_names.pickle, data_type: tuple\n",
      "path: /home/devspace/project/data/output_feature_finalizer/measurement_data_list.pickle, data_type: list\n",
      "path: /home/devspace/project/data/output_feature_finalizer/in_hospital_mortality_list.pickle, data_type: list\n",
      "\n",
      "All preprocess modules complete.\n",
      "Total preprocessing time = 529.46 seconds\n"
     ]
    }
   ],
   "source": [
    "preprocessed_resources = preprocessor.preprocess()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Pytorch Dataset object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Create the Dataset\n",
    "We import module `x19_mort_general_dataset` and use it along with files saved by the Preprocessor's Feature Finalizer module to insantiate a Pytorch Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lstm_adversarial_attack.x19_mort_general_dataset as xmd\n",
    "dataset = xmd.X19MGeneralDataset.from_feature_finalizer_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Examine the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples in dataset = 41951\n",
      "\n",
      "Type returned by dataset.__getitem__ = <class 'tuple'>\n",
      "\n",
      "Length of each tuple returned by dataset.__getitem__ = 2\n",
      "\n",
      "Object type, dimensionality, and datatype of each element in a tuple returned by dataset.__getitem__:\n",
      "((<class 'torch.Tensor'>, 2, torch.float32), (<class 'torch.Tensor'>, 0, torch.int64))\n",
      "\n",
      "input size (# columns) of each feature matrix is:\n",
      "19\n",
      "\n",
      "Distribution of input sequence lengths (# rows):\n",
      "length\tcounts\n",
      "[[    6     1]\n",
      " [   13     1]\n",
      " [   14     1]\n",
      " [   16     2]\n",
      " [   17     4]\n",
      " [   18     3]\n",
      " [   19     8]\n",
      " [   20    12]\n",
      " [   21    25]\n",
      " [   22    49]\n",
      " [   23    84]\n",
      " [   24   144]\n",
      " [   25   126]\n",
      " [   26   110]\n",
      " [   27    93]\n",
      " [   28    95]\n",
      " [   29    84]\n",
      " [   30    90]\n",
      " [   31    75]\n",
      " [   32    99]\n",
      " [   33    98]\n",
      " [   34   113]\n",
      " [   35   148]\n",
      " [   36   152]\n",
      " [   37   189]\n",
      " [   38   199]\n",
      " [   39   220]\n",
      " [   40   178]\n",
      " [   41   231]\n",
      " [   42   203]\n",
      " [   43   211]\n",
      " [   44   191]\n",
      " [   45   185]\n",
      " [   46   221]\n",
      " [   47   474]\n",
      " [   48 37832]]\n",
      "\n",
      "Label counts:\n",
      "value\tcounts\n",
      "[[    0 37338]\n",
      " [    1  4613]]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of samples in dataset = {len(dataset)}\\n\")\n",
    "print(f\"Type returned by dataset.__getitem__ = {type(dataset[0])}\\n\")\n",
    "print(\n",
    "    f\"Length of each tuple returned by dataset.__getitem__ = {len(dataset[0])}\"\n",
    ")\n",
    "print(\n",
    "    \"\\nObject type, dimensionality, and datatype of each element in a tuple\"\n",
    "    \" returned by dataset.__getitem__:\"\n",
    ")\n",
    "print(tuple([(type(item), item.dim(), item.dtype) for item in dataset[0]]))\n",
    "print(f\"\\ninput size (# columns) of each feature matrix is:\\n\"\n",
    "     f\"{np.unique([item.shape[1] for item in dataset[:][0]]).item()}\\n\")\n",
    "\n",
    "print(\"Distribution of input sequence lengths (# rows):\")\n",
    "print(\"length\\tcounts\")\n",
    "unique_sequence_lengths, sequence_length_counts = np.unique(\n",
    "    [item.shape[0] for item in dataset[:][0]], return_counts=True\n",
    ")\n",
    "print(\n",
    "    np.concatenate(\n",
    "        (\n",
    "            unique_sequence_lengths.reshape(-1, 1),\n",
    "            sequence_length_counts.reshape(-1, 1),\n",
    "        ),\n",
    "        axis=1,\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"\\nLabel counts:\")\n",
    "print(\"value\\tcounts\")\n",
    "unique_labels, label_counts = np.unique([dataset[:][1]], return_counts=True)\n",
    "print(\n",
    "    np.concatenate(\n",
    "        (unique_labels.reshape(-1, 1), label_counts.reshape(-1, 1)), axis=1\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Check for GPU\n",
    "Model hyperparameter tuning (along with training, and model attacks) is implemented in PyTorch, and we really need a GPU to run things in a reasonable amount of time. We check for a GPU a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cur_device is cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    cur_device = torch.device(\"cuda:0\")\n",
    "else:\n",
    "    cur_device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"cur_device is {cur_device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Instantiate and Examine TunerDriver\n",
    "We then instantiate a TunerDriver object with a device that is hopefully a GPU passed to its constructor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X19MLSTMTuningRanges(log_lstm_hidden_size=(5, 7),\n",
      "                     lstm_act_options=('ReLU', 'Tanh'),\n",
      "                     dropout=(0, 0.5),\n",
      "                     log_fc_hidden_size=(4, 8),\n",
      "                     fc_act_options=('ReLU', 'Tanh'),\n",
      "                     optimizer_options=('Adam', 'RMSprop', 'SGD'),\n",
      "                     learning_rate=(1e-05, 0.1),\n",
      "                     log_batch_size=(5, 8))\n"
     ]
    }
   ],
   "source": [
    "import lstm_adversarial_attack.tune_train.tuner_driver as td\n",
    "tuner_driver = td.TunerDriver(device=cur_device)\n",
    "pprint.pprint(tuner_driver.tuner.tuning_ranges)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint.pprint(tuner_driver.tuner.dataset[0][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_completed_study = tuner_driver(num_trials=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
