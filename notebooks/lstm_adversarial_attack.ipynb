{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# LSTM Time Series Deep Learning and Adversarial Attacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## 1. Background Information\n",
    "\n",
    "This project reproduces and expands upon work published in [1] and [2] on Long Short-Term Memory (LSTM) predictive models of Intensive Care Unit (ICU) patient outcomes, and adversarial attacks on those models. Following the approach of the previous studies, we use patient data from the Medical Information Mart for Intensive Care (MIMIC-III) database, and build a LSTM model with inputs consisting of 13 lab measurements and 6 vital signs. The prediction target is a binary variable representing in-hospital mortaliy. An adversarial attack algorithm with L1 regularization is then used to identify small perturbations which, when applied to a real, correctly-classified input features, caused a trained model to misclassify the perturbed input. After attacking a full dataset, susceptibility calculations were  performed to identify input feature space regions most vulnerable to adversarial attack.\n",
    "\n",
    "Aspects of the current work that expand upon the previous studies include faster data preprocessing algorithms; extensive hyperparameter tuning of both the predictive model and attack algorithm; improved performance of the predictive model; implementation of a GPU-compatible attack algorithm that enables attacking samples in batches; and not halting the attack process upon finding a single adversarial perturbation for a sample, allowing the discovery of additional, lower loss adversarial perturbations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## 2. Confirm Development Environment Setup\n",
    "The code and instructions in this notebook assume you have completed all steps in the [How to run this project](https://github.com/duanegoodner/lstm_adversarial_attack/tree/main#3-how-to-run-this-project) section of the project [README](https://github.com/duanegoodner/lstm_adversarial_attack), and you are running this notebook using Jupyter Lab inside Docker container `lstm_aa_app`. Run the following tests to confirm the environment is set up correcly:\n",
    "\n",
    "### 2.1 Docker Containers\n",
    "\n",
    "From a **local terminal** (not in any docker container), run `docker ps --format \"table {{.ID}}\\t{{.Ports}}\\t{{.Names}}\"`. The output should include the following lines:\n",
    "\n",
    "```\n",
    "CONTAINER ID   PORTS                                                                        NAMES\n",
    "152b6903a45b   127.0.0.1:6006->6006/tcp, 127.0.0.1:8888->8888/tcp, 127.0.0.1:2200->22/tcp   lstm_aa_app\n",
    "5520201f8420   0.0.0.0:5556->5432/tcp, :::5556->5432/tcp                                    postgres_optuna\n",
    "92e83589a4c8   0.0.0.0:5555->5432/tcp, :::5555->5432/tcp                                    postgres_mimiciii\n",
    "```\n",
    "Python code will run in `lstm_aa_app`. An instance of PostgreSQL in `postgres_mmimiciii` will hold MIMIC-III raw data for our model, and databases in `postgres_optuna` will store data from studies used to tune hyperparameters of our predictive model and adversarial attack model.\n",
    "\n",
    "### 2.2 Python Interpreter and IPython Kernel\n",
    "\n",
    "Runt the following quick tests to confirm which Python interpreter and IPython kernel we are using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/devspace/env/bin/python\n"
     ]
    }
   ],
   "source": [
    "!which python\n",
    "# Output should be: /home/devspace/env/bin/python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/gen_user/.local/share/jupyter/runtime/kernel-81090020-f042-4206-bf59-edb19b0a5814.json'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core.getipython import get_ipython\n",
    "get_ipython().kernel.config[\"IPKernelApp\"][\"connection_file\"]\n",
    "# Outptut should be similar to: '/home/gen_user/.local/share/jupyter/runtime/kernel-v2-26202uI0Vk7x2nHkK.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Test Database Connections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cell to test the MIMIC-III database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully connected to MIMIC-III database.\n",
      "Connection to MIMIC-III database successfully closed.\n"
     ]
    }
   ],
   "source": [
    "!python /home/devspace/project/src/lstm_adversarial_attack/query_db/test_mimiciii_db.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then run test queries on the databases that will handle hyperparameter tuning data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_tuning database successfully queried.\n",
      "Found 11 tuning studies.\n",
      "attack_tuning database successfully queried.\n",
      "Found 21 tuning studies.\n"
     ]
    }
   ],
   "source": [
    "!python /home/devspace/project/src/lstm_adversarial_attack/tuning_db/test_tuning_study_dbs.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Check for GPU\n",
    "\n",
    "The PyTorch code in our project will run much faster on a GPU than it will on a CPU. Let's find out if we have GPU access:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Change Working Directory\n",
    "Many of the code cells in this notebook use relative paths and assume we are in directory `/home/devspace/project/src/lstm_adversarial_attack`, so let's change to that directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/devspace/project/src/lstm_adversarial_attack\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(\"/home/devspace/project/src/lstm_adversarial_attack\")\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Project Structure\n",
    "Our `docker-compose.yml` maps the local project root directory to `/home/devspace/project` in the container. Run the following cell for an overview of our project layout. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34m/home/devspace/project\u001b[0m\n",
      "├── \u001b[01;32mREADME.md\u001b[0m\n",
      "├── \u001b[01;32mconfig.toml\u001b[0m\n",
      "├── \u001b[01;34mdata\u001b[0m\n",
      "├── \u001b[01;34mdocker\u001b[0m\n",
      "├── \u001b[01;34mdocs\u001b[0m\n",
      "├── \u001b[01;34mlogs\u001b[0m\n",
      "├── \u001b[01;34mnotebooks\u001b[0m\n",
      "└── \u001b[01;34msrc\u001b[0m\n",
      "\n",
      "6 directories, 2 files\n"
     ]
    }
   ],
   "source": [
    "!tree -L 1 /home/devspace/project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 `src/`\n",
    "The contents of `/home/devspace/project/src/lstm_adversarial_attack` are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34m/home/devspace/project/src/lstm_adversarial_attack\u001b[0m\n",
      "├── \u001b[01;34m__pycache__\u001b[0m\n",
      "├── \u001b[01;34mattack\u001b[0m\n",
      "├── \u001b[01;34mattack_analysis\u001b[0m\n",
      "├── \u001b[01;34mconfig\u001b[0m\n",
      "├── \u001b[01;34mdataset\u001b[0m\n",
      "├── \u001b[01;34mmodel\u001b[0m\n",
      "├── \u001b[01;34mpreprocess\u001b[0m\n",
      "├── \u001b[01;34mquery_db\u001b[0m\n",
      "├── \u001b[01;34mtuning_db\u001b[0m\n",
      "└── \u001b[01;34mutils\u001b[0m\n",
      "\n",
      "10 directories\n"
     ]
    }
   ],
   "source": [
    "!tree -d -L 1 /home/devspace/project/src/lstm_adversarial_attack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Code in the sub-directories listed above forms our project pipeline: \n",
    " * **query_db** runs .sql queries to extract patient lab, vital sign, and in-hospital mortality data from the MIMIC-III PostgreSQL database.\n",
    " * **preprocess** transforms .sql query output into a form that can be input to PyTorch models. \n",
    " * **model** tunes and trains a PyTorch model for predicting in-hospital mortality based on lab and vital sign time-series data.\n",
    " * **attack** tunes and trains a PyTorch attack model that generates adversarial examples for the predictive model.\n",
    " * **attack_analysis** generates plots for visualizing characteristics of adversarial examples found by the attack model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 `notebook_helpers`\n",
    "\n",
    "The `utils/notebook_helpers` module contains functions and classes to help streamline the data pipeline when running the project in a Jupyter notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils.notebook_helpers as nh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 `data/`\n",
    "\n",
    "For each of the critical directories under `src/lstm_adversarial_attack/`, there is a corresponding directory under `data/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34m/home/devspace/project/data\u001b[0m\n",
      "├── \u001b[01;34mattack\u001b[0m\n",
      "├── \u001b[01;34mattack_analyses_old\u001b[0m\n",
      "├── \u001b[01;34mattack_analysis\u001b[0m\n",
      "├── \u001b[01;34mexample_data\u001b[0m\n",
      "├── \u001b[01;34mmodel\u001b[0m\n",
      "├── \u001b[01;34mpreprocess\u001b[0m\n",
      "└── \u001b[01;34mquery_db\u001b[0m\n",
      "\n",
      "7 directories, 0 files\n"
     ]
    }
   ],
   "source": [
    "!tree -L 1 /home/devspace/project/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 `config.toml`\n",
    "Project configuration variables are set in the `config.toml` file. We can use `CONFIG_READER` and `CONFIG_MODIFIER` from the `config` sub-package to read and write to the `config.toml` file. The following code cell demonstrates how we can read / write `config.tomll` values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original value: 1234\n",
      "Value changed to: 2024\n",
      "Final value: 1234\n"
     ]
    }
   ],
   "source": [
    "orig_kfold_random_seed = nh.get_config_value(\"model.tuner_driver.kfold_random_seed\")\n",
    "print(f\"Original value: {orig_kfold_random_seed}\")\n",
    "\n",
    "nh.set_config_value(\"model.tuner_driver.kfold_random_seed\", 2024)\n",
    "modified_kfold_random_seed = nh.get_config_value(\"model.tuner_driver.kfold_random_seed\")\n",
    "print(f\"Value changed to: {modified_kfold_random_seed}\")\n",
    "\n",
    "nh.set_config_value(\"model.tuner_driver.kfold_random_seed\", orig_kfold_random_seed)\n",
    "\n",
    "final_kfold_random_seed = nh.get_config_value(\"model.tuner_driver.kfold_random_seed\")\n",
    "print(f\"Final value: {final_kfold_random_seed}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Storing Sub-package and Module Session IDs\n",
    "We will use an instance of `notebook_helpers.SessionIDs` to store the session IDs from the particular `modules` and `sub-package` run sessions that we want to send on through our pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'trainer_eval_general_logging_metrics': ['accuracy', 'auc', 'f1', 'precision', 'recall', 'validation_loss'], 'trainer_eval_tensorboard_metrics': ['auc', 'f1', 'precision', 'recall', 'validation_loss'], 'trainer': {'random_seed': 12345678}, 'tuner_driver': {'num_trials': 60, 'num_folds': 5, 'num_cv_epochs': 10, 'epochs_per_fold': 5, 'kfold_random_seed': 1234, 'performance_metric': 'validation_loss', 'optimization_direction_label': 'minimize', 'tuning_output_dir': 'data/model/tuning', 'pruner_name': 'MedianPruner', 'sampler_name': 'TPESampler', 'db_env_var_name': 'MODEL_TUNING_DB_NAME', 'fold_class_name': 'StratifiedKFold', 'collate_fn_name': 'x19m_collate_fn', 'cv_mean_tensorboard_metrics': ['accuracy', 'auc', 'f1', 'precision', 'recall', 'validation_loss'], 'tuning_ranges': {'log_lstm_hidden_size': [5, 7], 'lstm_act_options': ['ReLU', 'Tanh'], 'dropout': [0.0, 0.5], 'log_fc_hidden_size': [4, 8], 'fc_act_options': ['ReLU', 'Tanh'], 'optimizer_options': ['Adam', 'RMSprop', 'SGD'], 'learning_rate': [1e-05, 0.1], 'log_batch_size': [5, 8]}, 'pruner_kwargs': {'n_startup_trials': 5, 'n_warmup_steps': 3}, 'sampler_kwargs': {}}, 'cv_driver_settings': {'collate_fn_name': 'x19m_collate_fn', 'epochs_per_fold': 1000, 'eval_interval': 10, 'fold_class_name': 'StratifiedKFold', 'kfold_random_seed': 20240807, 'num_folds': 5, 'single_fold_eval_fraction': 0.2}, 'attr_display_labels': {'accuracy': 'accuracy', 'auc': 'AUC', 'f1': 'F1', 'precision': 'precison', 'recall': 'recall', 'validation_loss': '_validation_loss'}}\n"
     ]
    }
   ],
   "source": [
    "result = nh.get_config_value(\"model\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'utils.notebook_helpers' has no attribute 'PipelineInfo'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpprint\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m session_ids \u001b[38;5;241m=\u001b[39m \u001b[43mnh\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPipelineInfo\u001b[49m()\n\u001b[1;32m      3\u001b[0m pprint\u001b[38;5;241m.\u001b[39mpprint(session_ids)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'utils.notebook_helpers' has no attribute 'PipelineInfo'"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "session_ids = nh.PipelineInfo()\n",
    "pprint.pprint(session_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Database Queries\n",
    "\n",
    "Raw ICU patient data can be extracted from the MIMIC-III database using modified versions of four `.sql`queries from the [MIT-LCP mimic-code repository](https://github.com/MIT-LCP/mimic-code/tree/main/mimic-iii/concepts/pivot).\n",
    "\n",
    "### 4.1 Running the Queries\n",
    "\n",
    "We connect to the database and execute the queries by running the [\\_\\_main__](../src/lstm_adversarial_attack/query_db/__main__.py) module of the [query_db](../src/lstm_adversarial_attack/query_db/\\_\\_init__.py) sub-package. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m query_db --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m query_db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Storing Query Session ID\n",
    "\n",
    "We need to store ID of the database query session that will provide input to the next stage of our data pipeline. If we want to use the most recent query, assign `None` to custom_db_query ID. If we want to use a different query ID, assign that ID value to `custom_query_session_id` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: assign either None or specific query_session_id to use.\n",
    "# If use `None`, Prefilter will use most recently created query session.\n",
    "custom_query_session_id = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_ids.set(\"db_queries\", custom_query_session_id)\n",
    "\n",
    "print(\"\\nCurrent values in SessionIDs container:\")\n",
    "pprint.pprint(session_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Implementation Details\n",
    "\n",
    "We will use the [`preprocess`](../src/lstm_adversarial_attack/preprocess/__init__.py) sub-package to transform information from the `.csv` files output by the `.sql` queries into numpy arrays (which can then be easily converted into PyTorch tensors). Running this sub-package instantiates a `Preprocessor` object with a `.preprocess_modules` attribute assigned by the following code in  [`preprocessor.py`](../src/lstm_adversarial_attack/preprocess/preprocessor.py):\n",
    "\n",
    "```\n",
    "self.preprocess_modules = [\n",
    "            prf.Prefilter(),\n",
    "            imc.ICUStayMeasurementCombiner(),\n",
    "            slb.FullAdmissionListBuilder(),\n",
    "            fb.FeatureBuilder(),\n",
    "            ff.FeatureFinalizer(),\n",
    "        ]\n",
    "```\n",
    "Each element of the `.preprocess_modules` attribute is a subclass of [`PreprocessModule`](../src/lstm_adversarial_attack/preprocess/preprocess_module.py).\n",
    "\n",
    "* [`Prefilter`](../src/lstm_adversarial_attack/preprocess/prefilter.py) reads the database query outputs into Pandas Dataframes, removes all data related to patients younger than 18 years in age, ensures consistent column naming formats, and takes care of datatype details.\n",
    "* [`ICUStayMeasurementCombiner`](../src/lstm_adversarial_attack/preprocess/icustay_measurement_combiner.py) performs various joins (aka \"merges\" in the language of Pandas) to combine lab and vital sign measurement data with ICU stay data.\n",
    "* [`FullAdmissionListBuilder`](../src/lstm_adversarial_attack/preprocess/sample_list_builder.py) generates a list consisting of one FullAdmissionData object per ICU stay. The attributes of a FullAdmissionData object include ICU stay info, and a dataframe containing the measurement and timestamp data for all vital sign and lab data associated with the ICU stay.\n",
    "* [`FeatureBuilder`](../src/lstm_adversarial_attack/preprocess/feature_builder.py) resamples the time series datafame to one-hour intervals, imputes missing data, winsorizes measurement values (with cutoffs at the 5th and 95th global percentiles), and normalizes the measuremnt values so all data are between 0 and 1.\n",
    "* [`FeatureFinalizer`](../src/lstm_adversarial_attack/preprocess/feature_finalizer.py) selects the data observation time window (default starts at hospital admission time and ends 48 hours after admission). This module outputs the entire dataset features as a list of numpy arrays, and the mortality labels as a list of integers. These data structures (saved as .pickle files) will be convenient starting points when the `tune_train` and `attack` sub-packages need to create PyTorch Datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Set Preprocess Config Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nh.set_config_value(\"preprocess\", {'min_age': 18,\n",
    " 'min_los_hospital': 1,\n",
    " 'min_los_icu': 1,\n",
    " 'bg_data_cols': ['potassium', 'calcium', 'ph', 'pco2', 'lactate'],\n",
    " 'lab_data_cols': ['albumin',\n",
    "  'bun',\n",
    "  'creatinine',\n",
    "  'sodium',\n",
    "  'bicarbonate',\n",
    "  'platelet',\n",
    "  'glucose',\n",
    "  'magnesium'],\n",
    " 'vital_data_cols': ['heartrate',\n",
    "  'sysbp',\n",
    "  'diasbp',\n",
    "  'tempc',\n",
    "  'resprate',\n",
    "  'spo2'],\n",
    " 'winsorize_low': '5%',\n",
    " 'winsorize_high': '95%',\n",
    " 'resample_interpolation_method': 'linear',\n",
    " 'resample_limit_direction': 'both',\n",
    " 'min_observation_hours': 48,\n",
    " 'observation_window_hours': 48,\n",
    " 'observation_window_start': 'intime'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Run the Preprocess Modules\n",
    "\n",
    "We can run all preprocessing modules by executing the `preprocess` sub-packages `__main__`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m preprocess --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m preprocess -d {session_ids.db_queries}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO assing value if needed\n",
    "custom_preprocess_session_id = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_ids.set(\"preprocess\", custom_preprocess_session_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_session_id = get_session_id(custom_preprocess_session_id, \"preprocess.output_root\")\n",
    "preprocess_session_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Summarize Feature Finalizer Output\n",
    "We can get information about the array shape and value distributions of the preprocessed using the `preprocess` sub-package's `inspect_feature_finalizer` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python preprocess/inspect_feature_finalizer_output.py --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python preprocess/inspect_feature_finalizer_output.py -p {session_ids.preprocess}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each sample in the FeatureFinalizer output is from a unique ICU stay, and consists of a 2D matrix of input features and a binary class label. Each column in a feature matrix corresponds to a particular lab or vital sign measurement, and each row in a feature matrix corresponds to the number of hours elapsed after a patient's hospital admission time. A class label of 1 indicates an in-hospital mortality event.\n",
    "\n",
    "When preprocessor parameters in `config.toml` are set to default values, the FeatureFinalizer output consists of 37832 samples, and the shape of all input feature arrays is 48 x 19, and approximately 11% of the preprocessed samples have class label = 1. Later, when we tune and train our predictive model, we will use oversampling techniques to deal with the significant class imbalance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 Preprocessing Time\n",
    "\n",
    "On an Intel i7-13700K CPU, the above preprocessing work takes approximately 3.9 minutes. Achieving the same transformations on the same machine with preprocessing code from [[1](#References)] takes approximately 45 minutes. This time difference is largely due to the fact that the current project preprocess subpackage avoids using  unnecessary loops and relies heavily vectorized Pandas and Numpy operations.\n",
    "\n",
    "Additional time reduction could be achieved by parellelizing the preprocess computations with tools such as [pandaparallel](https://github.com/nalepae/pandarallel) or [pyspark](https://spark.apache.org/docs/3.3.1/api/python/index.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Architecture\n",
    "\n",
    "The starting point for our predictive model is based on the model in [1] and consists of the following layers:\n",
    "\n",
    "| Layer # | Description        | Input Shape                            | Parameters          | Output Shape           | Activation       |\n",
    "| ------- | ------------------ | -------------------------------------- | ------------------- | ---------------------- | ---------------- |\n",
    "| 1       | Bidirectional LSTM | (b, t<sub>max</sub> = 48, n<sub>meas</sub> = 19) | n<sub>LSTM</sub>    | (b, 2n<sub>LSTM</sub>) | a<sub>LSTM</sub> |\n",
    "| 2       | Dropoout           | (b, 2n<sub>LSTM</sub>)                 | P<sub>dropout</sub> | (b, 2n<sub>LSTM</sub>) | -                |\n",
    "| 3       | Fully Connected    | (b, 2n<sub>LSTM</sub>)                 | n<sub>FC</sub>      | (b, n<sub>FC</sub>)    | a<sub>FC</sub>   |\n",
    "| 4       | Output             | (b, n<sub>FC</sub>)                    | n<sub>out</sub> = 2 | (b, n<sub>out</sub>    | a<sub>out</sub>  |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameters from the above table are defined as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Parameter           | Description                                             |\n",
    "| ------------------- | ------------------------------------------------------- |\n",
    "| b                   | Batch size                                              |\n",
    "| t<sub>max</sub>     | Maximum input sequence length                           |\n",
    "| n<sub>meas</sub>    | Number of patient measurement types                     |\n",
    "| n<sub>LSTM</sub>    | Number of features in a LSTM hidden state               |\n",
    "| a<sub>LSTM</sub>    | Activation function for the LSTM output                 |\n",
    "| P<sub>dropout</sub> | Dropout probablity                                      |\n",
    "| n<sub>FC</sub>      | Numbef of nodes in the fully connected layer            |\n",
    "| a<sub>FC</sub>      | Activation function for the fully connected layer ouput |\n",
    "| n<sub>out</sub>     | Number of nodes in the output layer                     |\n",
    "| a<sub>out</sub>     | Activation function for the output layer                |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that n<sub>meas</sub>, n<sub>out</sub>, abd s<sub>max</sub> are fixed. We have chosen to always use all 19 patient measurement types, and our classification problem always has two classes. In our current data pipeline, data collected outside of a specified time window are removed during the final preprocessing phase. If we want the observation window to be tunable, it would be helpful to move the `preprocess.feature_finalizer` module into the `tune_attack` sub-package."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Hyperparameter Tuning\n",
    "\n",
    "### 7.1 Architectural hyperparameters\n",
    "\n",
    "The following table lists the ranges architectural parameters to be explored during hyperparameter tuning.\n",
    "\n",
    "| Parameter           | Tuning Type  | Values                            |\n",
    "| ------------------- | ------------ | --------------------------------- |\n",
    "| b                   | Discrete     | 2<sup>k</sup> , k = 5, 6, 7, 8    |                    \n",
    "| h<sub>LSTM</sub>    | Discrete     | 2<sup>k</sup> , k = 5, 6, 7       |\n",
    "| a<sub>LSTM</sub>    | Discrete     | ReLU, Tanh                        |\n",
    "| P<sub>dropout</sub> | Continuous   | 0.000 $\\textemdash$ 0.5000        |\n",
    "| h<sub>FC</sub>      | Discrete     | 2<sup>k</sup> , k = 4, 5, 6, 7, 8 |\n",
    "| a<sub>FC</sub>      | Discrete     | ReLU, Tanh                        |\n",
    "\n",
    "\n",
    "### 7.2 Trainer hyperparameters\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During hyperparameter tuning, we also explore different training optimization algorithms and learning rates.\n",
    "\n",
    "| Parameter     | Tuning Type | Values             |\n",
    "| ------------- | ----------- | ------------------ |\n",
    "| Optimizer     | Discrete    | SGD, RMSprop, Adam |\n",
    "| Learning Rate | Continuous  | 1e-5 - 1e-1        |\n",
    "\n",
    "When using the Adam optimizer, we always use the Pytorch default values of $\\beta_1 = 0.9, \\beta_2 = 0.999, \\epsilon = 10^{-8}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 Implementation Details\n",
    "The [`HyperParameterTuner`](../src/lstm_adversarial_attack/tune_train/hyperparameter_tuner.py) class in the [`model`](../src/lstm_adversarial_attack/model/__init__.py) sub-package implements a cross-validation tuning scheme that utilizes the [Optuna](https://optuna.org/) framework. The boundaries of hyperparameter space to explore during tuning are set in the `[model.tuner_driver.tuning_ranges]` section of the projectr `config.toml` file.\n",
    "\n",
    "Other model hyperparameter tuning settings are also configured under `[model.tuner_driver]`. In the standard configuration, a PyTorch [`StratifiedKFold`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html) generator is used to assign samples to each fold. When selecting samples for each training batch, we use a [`DataLoader`](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) with a [`WeightedRandomSampler`](https://pytorch.org/docs/stable/data.html#torch.utils.data.WeightedRandomSampler) to oversample from the minority class (label = 1). For a given set of hyperparameters, the [`HyperParameterTuner.objective_fn`](../src/lstm_adversarial_attack/tune_train/hyperparaemter_tuner.py) method returns the mean validation loss across the K folds, and this mean loss is used as a minimization target by an Optuna [`TPESampler`](https://optuna.readthedocs.io/en/stable/reference/samplers/generated/optuna.samplers.TPESampler.html) to select new sets of hyperparameters for additional trials. [`HyperParameterTuner`](../src/lstm_adversarial_attack/tune_train/hyperparaemter_tuner.py) also uses an Optuna [`MedianPruner`](https://optuna.readthedocs.io/en/stable/reference/generated/optuna.pruners.MedianPruner.html) to stop unpromising trials early.\n",
    "\n",
    "### 7.4 Model Tuning Configuration Settings\n",
    "Set `[model.tuner_driver]` configuration values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nh.set_config_value(\"model.tuner_driver\", {'num_trials': 60,\n",
    " 'num_folds': 5,\n",
    " 'num_cv_epochs': 10,\n",
    " 'epochs_per_fold': 5,\n",
    " 'kfold_random_seed': 1234,\n",
    " 'performance_metric': 'validation_loss',\n",
    " 'optimization_direction_label': 'minimize',\n",
    " 'tuning_output_dir': 'data/model/tuning',\n",
    " 'pruner_name': 'MedianPruner',\n",
    " 'sampler_name': 'TPESampler',\n",
    " 'db_env_var_name': 'MODEL_TUNING_DB_NAME',\n",
    " 'fold_class_name': 'StratifiedKFold',\n",
    " 'collate_fn_name': 'x19m_collate_fn',\n",
    " 'cv_mean_tensorboard_metrics': ['accuracy',\n",
    "  'auc',\n",
    "  'f1',\n",
    "  'precision',\n",
    "  'recall',\n",
    "  'validation_loss'],\n",
    " 'tuning_ranges': {'log_lstm_hidden_size': [5, 7],\n",
    "  'lstm_act_options': ['ReLU', 'Tanh'],\n",
    "  'dropout': [0.0, 0.5],\n",
    "  'log_fc_hidden_size': [4, 8],\n",
    "  'fc_act_options': ['ReLU', 'Tanh'],\n",
    "  'optimizer_options': ['Adam', 'RMSprop', 'SGD'],\n",
    "  'learning_rate': [1e-05, 0.1],\n",
    "  'log_batch_size': [5, 8]},\n",
    " 'pruner_kwargs': {'n_startup_trials': 5, 'n_warmup_steps': 3},\n",
    " 'sampler_kwargs': {}})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `[model.tuner_driver]` secton of the `config.toml` includes parameters that determine the number of tuning trials, cross-validation folds, and epochs. With the values set above, we run an Optuna study with 60 trials. Each trial uses 5-fold cross-validation, and we run `num_cv_epochs * epochs_per_fold = 10 * 5 = 50` total epochs on each fold. (NOTE: Consider changing tname of `epochs_per_fold` to something less confusing.)\n",
    "\n",
    "### 7.5 Start a New Hyperparameter Tuning Study\n",
    "Before starting, a few things to note:\n",
    "* Depending your GPU compute power, running the full 30 trials could take 2 - 20 hours.\n",
    "* Results will be saved to a newly created directory (with a timestamp-based name) under `data/model/tuning/<tuning_session_id>`. \n",
    "* If the study is stopped early (via CTRL-C or the Jupyter Stop button), learning from whatever trials have completed up to that point will be saved.\n",
    "* While the tuning trials are running, read ahead to the notebook section with instructions on how to monitor progress in Tensorboard.\n",
    "\n",
    "We can start a new hyperparaemter tuning session by running the `tune_new` module in the `model` sub-package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python model/tune_new.py --help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since terminal output during tuning can be very long, we will use the  `-r` option to redirect output to a log file and keep our notebook tidy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python model/tune_new.py -p {session_ids.preprocess} -r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.6 Resume an Existing Hyperparameter Tuning Study\n",
    "We can run additional trials for an existing study using the `model` sub-package's `tune_resume` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python model/tune_resume.py --help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to continue a study, we can un-comment each of the next two cells, and assign a value to `model_tuning_id_for_continuation`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_tuning_id_for_continuation = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python model/tune_resume.py -t {model_tuning_id_for_continuation} -r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.7 Monitor Tuning Progress with Tensorboard\n",
    "\n",
    "While we are tuning hyperparameters, we can monitor results in Tensorboard. Use Jupyter Lab to open a new terminal, and run:\n",
    "\n",
    "```\n",
    "tensorboard --logdir=/home/devspace/project/data/model/tuning/<tuning-session-id>/tensorboard --host=0.0.0.0\n",
    "```\n",
    "\n",
    "Then, in your browser, go to: `http://localhost:6006/`. You should see something like the screenshot below.  The x-axis for all plots is epoch number. (Unfortunately, there is no good way to add axis labels in Tensorboard.) Note: `<tuning-session-ID>` is included in the output when running the `tune_new` and/or `tune_resume` modules.\n",
    "\n",
    "Here is an example screen-shot of plots displayed in Tensorboard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![tensorboard_image](images/tensorboard_model_tuning_50_epochs.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.9 Set ID of Tuning Session to Use for Training, and View Session's Best Hyperparameters\n",
    "When we are done tuning, we set the ID of the model tuning session to use as input to model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO assign value if not using most recently created tuning session\n",
    "custom_model_tuning_id_for_training = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_ids.set(\"model_tuning\", custom_model_tuning_id_for_training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we view the best set of hyperparameters from this tuning session using the `view_best_model_hyperparameters` module of the `model` sub-package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python model/view_best_model_hyperparameters.py --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python model/view_best_model_hyperparameters.py -t {model_tuning_id_for_training}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Model Training\n",
    "For model hyperparameter tuning described in the previous section, we typically run ~50 epochs per fold (in the interest of reducing compute requirements). Based on the validation loss, AUC, and F1 curves from tuning trials, it appears that predictive performance could be improved by training for a larger number of epochs. We now run another round of Stratified K-fold cross-validation with our best set of parameters with a larger number of epochs.\n",
    "\n",
    "### 8.1 Notes on our Method\n",
    "* We are using \"flat\" cross-validation (as was done in previous studies on this dataset). This method computationally less expensive than nested cross-validation. Flat cross-validation has the potential to overestimate of model performance. In many cases the magnitude of overestimation is small. We also mitigate this effect by using a different set of (randomly generated) fold assignments than was used for hyperparameter tuning. \n",
    "* By selecting our hyperparameters based on the smaller number of epochs (100), we favor models that are faster to to train. It is possible that using a larger number of epochs in the tuning runs would have yielded a different (and better) set of \"best\" hyperparameters, but would also be computationally more expensive.\n",
    "\n",
    "### 8.2 Cross-Validation Training Settings\n",
    "Settings used during model training are specified in the `[model.cv_driver_settings]` section of the `config.toml` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG_MODIFIER.set(\"model.cv_driver_settings\", {'collate_fn_name': 'x19m_collate_fn',\n",
    " 'epochs_per_fold': 1000,\n",
    " 'eval_interval': 10,\n",
    " 'fold_class_name': 'StratifiedKFold',\n",
    " 'kfold_random_seed': 20240807,\n",
    " 'num_folds': 5,\n",
    " 'single_fold_eval_fraction': 0.2})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3 Run Cross-Validation Training\n",
    "We can begin a training session by running the `train` module in the `model` sub-package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python model/train.py --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python model/train.py -t {model_tuning_id_for_training} -r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.4 Monitor Cross-Validation Progress in Tensorbard\n",
    "\n",
    "To view training curves in tensorboard, use Jupyter Lab to open a new terminal, and run:\n",
    "```\n",
    "tensorboard --logdir /home/devspace/project/data/model/tuning/<cross-validation-training-session-id>/tensorboard --host=0.0.0.0\n",
    "```\n",
    "\n",
    "Then, go to http://localhost:6006 in your browser.\n",
    "\n",
    "This Tensorboard screenshot was taken at the end of a 5-fold, 1000 epoch per fold cross-validation run.\n",
    "![tensorboard_image](images/tensorboard_model_training_1000_epochs.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.5 Model Training Behavior: Continued Improvement at High Epoch Counts\n",
    "\n",
    "The above AUC and validation loss curves show continued (though diminishing) improvement in predictive performance during the entire 1000 epochs. The fact that we do not observe any sign of overfitting at such a large number of epochs is somewhat unusual. A likely cause of this behavior is the `WeightedRandomSampler` used in our training `DataLoaders`. Samples with our minority class label (`mortality = 1`) only represent ~15% of the total dataset. To deal with this imbalanced dataset, we oversample from the minority class and undersample from the majority class when creating batches of samples for training. In our current implementation, some samples from the majority class go unseen by the `StandardModelTrainer` for a large number of epochs. The number of unseen samples slowly dwindles (and the amount of information available for training slowly increases), even at very high epoch counts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.6 Summarize Model Training Results\n",
    "We can run the `model` sub-package's `view_model_training_summary` module to summarize each fold's best-performing checkpoint as well as the means and standard deviations of performance metrics across all folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO assign value if not using most recently created tuning session\n",
    "custom_cv_training_id_for_summary = 20240803135302155971"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_training_id_for_summary = get_session_id(custom_cv_training_id_for_summary, \"model.cv_driver.output_dir\")\n",
    "cv_training_id_for_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python model/view_model_training_summary.py --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python model/view_model_training_summary.py -t {cv_training_id_for_summary}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.7 Comparing Training Results with Prior Studies' Predictive Models\n",
    "\n",
    "The table below compares the predictive performance of the LSTM model in this work with other LSTM-based models using the same dataset. The current model shows the best predictive performance among all models in the table based on AUC and F1 scores. \n",
    "\n",
    "\n",
    "|  | Authors       | Model      | Input Features | AUC             | F1              | Precision       | Recall          |\n",
    "|-|------------|------------|----------------|-----------------|-----------------|-----------------|-----------------|\n",
    "|1 |Sun et al.  | LSTM-128 + FC-32 + FC-2 | [13 labs, 6 vitals] x 48 hr  | 0.9094 (0.0053) | 0.5429 (0.0194) | 0.4100 (0.0272) | 0.8071 (0.0269) |\n",
    "|2 |Tang et al. | LSTM-256 + FC-2 | [13 labs, 6 vitals] x 48 hr + demographic data  | 0.949 (0.003) | 0.623 (0.012) | \n",
    "| 3|Tang et al. | CNN + LSTM-256 + FC-2 | [13 labs, 6 vitals] x 48 hr + demographic data | 0.940 (0.0071) | 0.633 (0.031) | \n",
    "|4 |Tang et al. | CNN + LSTM-256 + FC-2 | [13 labs, 6 vitals] x 48 hr | 0.933 (0.006) | 0.587 (0.025) |\n",
    "|5 |Tang et al. | LSTM-256 + FC-2 | [13 labs, 6 vitals] x 48 hr | 0.907 (0.006) | 0.526 (0.013) |\n",
    "|6 |This work   | LSTM-128 + FC-16 + FC-2 | [13 labs, 6 vitals] x 48 hr  | 0.9657 (0.0035) | 0.9669 (0.0038) | 0.9888 (0.0009) | 0.9459 (0.0072) |\n",
    "\n",
    "> **Notes** LSTM-X indicates an LSTM with X hidden layers. FC-X indicates a fully connected layer with an output size of X. All LSTMs are bidirectional. The demographic data used in studies #2 and #3 was obtained from MIMIC-III.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9 Attack Hyperparameter Tuning\n",
    "\n",
    "Before running an attack on the entire dataset, we tune attack hyperparameters with help from `optuna`. Our approach here is not as rigourous as the one we used for predictive model tuning. We use only a fraction of the total dataset for tuning, and do not perform cross-validation.\n",
    "\n",
    "### 9.1 Set ID of Training Session to Use as Input for Attack Tuning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_cv_training_id_for_attack_tuning = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if custom_cv_training_id_for_attack_tuning is None:\n",
    "    cv_training_id_for_attack_tuning = cv_training_id_for_summary\n",
    "else:\n",
    "    cv_training_id_for_attack_tuning = custom_cv_training_id_for_attack_tuning\n",
    "    \n",
    "cv_training_id_for_attack_tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2 Attack Hyperparameter Tuning Config Settings\n",
    "\n",
    "Settings that affect attack hyperparameter tuning are under `[attack.tuning.ranges]` and `[attack.tuner_driver_settings]`in the `config.toml`. We can view the current values of these settings using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG_MODIFIER.set(\"attack.tuning.ranges\", {'kappa': [0.0, 2.0],\n",
    " 'lambda_1': [1e-07, 1.0],\n",
    " 'learning_rate': [1e-05, 1.0],\n",
    " 'log_batch_size': [5, 7],\n",
    " 'optimizer_options': ['Adam', 'RMSprop', 'SGD']})\n",
    "\n",
    "CONFIG_MODIFIER.set(\"attack.tuner_driver_settings\", {'db_env_var_name': 'ATTACK_TUNING_DB_NAME',\n",
    " 'num_trials': 75,\n",
    " 'epochs_per_batch': 1000,\n",
    " 'max_num_samples': 1028,\n",
    " 'sample_selection_seed': 2023,\n",
    " 'pruner_name': 'MedianPruner',\n",
    " 'sampler_name': 'TPESampler',\n",
    " 'objective_name': 'sparse_small',\n",
    " 'max_perts': 0,\n",
    " 'attack_misclassified_samples': False,\n",
    " 'objective_extra_kwargs': {},\n",
    " 'pruner_kwargs': {},\n",
    " 'sampler_kwargs': {}})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `max_num_samples` parameter specifies the number of samples to be considered for attack. However, samples that are misclassified by the target model are not attacked, so the actual number of samples used for tuning will be slightly lower.\n",
    "\n",
    "### 9.2 Adversarial Example Quality Scores\n",
    "\n",
    "Each trial in an attack tuning study runs `epochs_per_batch` attack iterations on each of the selected samples. Then, an adversarial example quality score is calculated for the lowest loss adversarial example each sample that has at least one associated adversarial example. A trial's score is the sum of these example quality scores. The `objective_name` parameter specifies the objective function used to calculate the quality scores.\n",
    " \n",
    " The following table summarizes how each of the available objective functions calculates the quality score of a single adversarial perturbation matrix $P_{adv}$.\n",
    "| Objective                                                   | Example Quality Score Formula                              |\n",
    "| ----------------------------------------------------------- | ------------------------------------------------------------ |\n",
    "| sparsity        | $1 - f_{nonzero}$ |\n",
    "| max_num_nonzero_perts | $if\\; n_{nonzero} < n_{critical}: 1, otherwise: 0$ |\n",
    "| sparse_small           | $sparsity\\;/\\;\\|P_{adv}\\|_1$ |\n",
    "| sparse_small_max       | $sparsity\\;/\\ max(|P_{adv}|)$  |\n",
    "\n",
    "where $n_{nonzero}$ is the number of non-zero elements in $P_{adv}$, $f_{nonzero}$ is the fraction of non-zero elements, $\\|P_{adv}\\|_1$ is the L1 norm, and $|P_{adv}|$ is the element-wise absolute value.\n",
    "\n",
    "### 9.3 Tune Attack with `sparse_small` Objective\n",
    "\n",
    "Although, `attack.tuner_driver_settings.objective_name` should already be set to `sparse_small_max`, let's set it again to be sure:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG_MODIFIER.set(\"attack.tuner_driver_settings.objective_name\", \"sparse_small\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can start a new attack hyperparameter tuning session with the `attack` sub-package's `tune_attack_new` module. If a tuning session is stopped early (via CTRL-C or the notebook Stop button), data from completed trials will be saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python attack/tune_attack_new.py --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python attack/tune_attack_new.py -t {cv_training_id_for_attack_tuning} -r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.4 Tune Attack with `sparse_small_max` Objective\n",
    "\n",
    "We change the objective name in our `config.toml` using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG_MODIFIER.set(\"attack.tuner_driver_settings.objective_name\", \"sparse_small_max\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python attack/tune_attack_new.py -t {cv_training_id_for_attack_tuning} -r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### 9.4 Resuming an Existing Attack Tuning Session\n",
    "The `tune_attack_resume` module can be used to run more trials as part of an existing attack tuning study. If we are resuming the most recently created attack tuning study, we can set `custom_attack_tuning_id_to_resume` in the next cell to `None`. Otherwise, set it to the ID of the session we want to resume."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: set this to ID of session to resume (OK to leave as None if resuming most recently created study)\n",
    "custom_attack_tuning_id_to_resume = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attack_tuning_id_to_resume = get_session_id(custom_attack_tuning_id_to_resume, \"attack.tuner_driver.output_dir\")\n",
    "attack_tuning_id_to_resume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python attack/tune_attack_resume.py --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python attack/tune_attack_resume.py -t {attack_tuning_id_to_resume} -r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Attacking the Full Dataset with Tuned Attack Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG_READER.get_value(\"attack.driver_settings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG_MODIFIER.set(\"attack.driver_settings\", {'epochs_per_batch': 1000,\n",
    " 'max_num_samples': 40000,\n",
    " 'sample_selection_seed': 2023,\n",
    " 'attack_misclassified_samples': False})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.1 Attack Using Best Hyperparameters from `sparse_small_max` Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python attack/attack.py -t {sparse_small_max_attack_tuning_id} -r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.2 Attack Using Best Hyperparameters from `sparse_small` Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python attack/attack.py -t {sparse_small_attack_tuning_id} -r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "<a id=\"ref_01\"></a>1. [Sun, M., Tang, F., Yi, J., Wang, F. and Zhou, J., 2018, July. Identify susceptible locations in medical records via adversarial attacks on deep predictive models. In *Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery & data mining* (pp. 793-801).](https://dl.acm.org/doi/10.1145/3219819.3219909)\n",
    "\n",
    "<a id=\"ref_02\">2.</a> [Tang, F., Xiao, C., Wang, F. and Zhou, J., 2018. Predictive modeling in urgent care: a comparative study of machine learning approaches. *Jamia Open*, *1*(1), pp.87-98.](https://academic.oup.com/jamiaopen/article/1/1/87/5032901)\n",
    "\n",
    "<a><a id=\"ref_03\">3.</a> </a>[Johnson, A., Pollard, T., and Mark, R. (2016) 'MIMIC-III Clinical Database' (version 1.4), *PhysioNet*.](https://doi.org/10.13026/C2XW26) \n",
    "\n",
    "<a id=\"ref_04\">4.</a> [Johnson, A. E. W., Pollard, T. J., Shen, L., Lehman, L. H., Feng, M., Ghassemi, M., Moody, B., Szolovits, P., Celi, L. A., & Mark, R. G. (2016). MIMIC-III, a freely accessible critical care database. Scientific Data, 3, 160035.](https://www.nature.com/articles/sdata201635)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
