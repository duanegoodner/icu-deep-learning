{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# LSTM Time Series Deep Learning and Adversarial Attacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## 1. Background Information\n",
    "\n",
    "This project reproduces and expands upon work published in [1] and [2] on Long Short-Term Memory (LSTM) predictive models of Intensive Care Unit (ICU) patient outcomes, and adversarial attacks on those models. Following the approach of the previous studies, we use patient data from the Medical Information Mart for Intensive Care (MIMIC-III) database, and build a LSTM model with inputs consisting of 13 lab measurements and 6 vital signs. The prediction target is a binary variable representing in-hospital mortaliy. An adversarial attack algorithm with L1 regularization is then used to identify small perturbations which, when applied to a real, correctly-classified input features, caused a trained model to misclassify the perturbed input. After attacking a full dataset, susceptibility calculations were  performed to identify input feature space regions most vulnerable to adversarial attack.\n",
    "\n",
    "Aspects of the current work that expand upon the previous studies include faster data preprocessing algorithms; extensive hyperparameter tuning of both the predictive model and attack algorithm; improved performance of the predictive model; implementation of a GPU-compatible attack algorithm that enables attacking samples in batches; and not halting the attack process upon finding a single adversarial perturbation for a sample, allowing the discovery of additional, lower loss adversarial perturbations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## 2. Confirm Development Environment Setup\n",
    "The code and instructions in this notebook assume you have completed all steps in the [How to run this project](https://github.com/duanegoodner/lstm_adversarial_attack/tree/main#3-how-to-run-this-project) section of the project [README](https://github.com/duanegoodner/lstm_adversarial_attack). Run the following tests to confirm the environment is set up correcly:\n",
    "\n",
    "### 2.1 Docker Containers\n",
    "\n",
    "From a **local terminal** (not in any docker container), run `docker ps --format \"table {{.ID}}\\t{{.Ports}}\\t{{.Names}}\"`. The output should include the following lines:\n",
    "\n",
    "```\n",
    "CONTAINER ID   PORTS                                                                        NAMES\n",
    "152b6903a45b   127.0.0.1:6006->6006/tcp, 127.0.0.1:8888->8888/tcp, 127.0.0.1:2200->22/tcp   lstm_aa_app\n",
    "5520201f8420   0.0.0.0:5556->5432/tcp, :::5556->5432/tcp                                    postgres_optuna\n",
    "92e83589a4c8   0.0.0.0:5555->5432/tcp, :::5555->5432/tcp                                    postgres_mimiciii\n",
    "```\n",
    "Python code will run in `lstm_aa_app`. An instance of PostgreSQL in `postgres_mmimiciii` will hold MIMIC-III raw data for our model, and databases in `postgres_optuna` will store data from studies used to tune hyperparameters of our predictive model and adversarial attack model.\n",
    "\n",
    "### 2.2 Python Interpreter and IPython Kernel\n",
    "\n",
    "Runt the following quick tests to confirm which Python interpreter and IPython kernel we are using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/devspace/env/bin/python\n"
     ]
    }
   ],
   "source": [
    "!which python\n",
    "# Output should be: /home/devspace/env/bin/python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/gen_user/.local/share/jupyter/runtime/kernel-4fdf88e0-c8db-43fe-9aa6-81fa68f13c8a.json'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core.getipython import get_ipython\n",
    "get_ipython().kernel.config[\"IPKernelApp\"][\"connection_file\"]\n",
    "# Outptut should be similar to: '/home/gen_user/.local/share/jupyter/runtime/kernel-v2-26202uI0Vk7x2nHkK.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Test Database Connections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cell to test the MIMIC-III database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully connected to MIMIC-III database.\n",
      "Connection to MIMIC-III database successfully closed.\n"
     ]
    }
   ],
   "source": [
    "!python /home/devspace/project/src/lstm_adversarial_attack/query_db/test_mimiciii_db.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then run test queries on the databases will handle hyperparameter tuning data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_tuning database successfully queried.\n",
      "Found 15 tuning studies.\n",
      "attack_tuning database successfully queried.\n",
      "Found 1 tuning studies.\n"
     ]
    }
   ],
   "source": [
    "!python /home/devspace/project/src/lstm_adversarial_attack/tuning_db/test_tuning_study_dbs.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Check for GPU\n",
    "\n",
    "The PyTorch code in our project will run much faster on a GPU than it will on a CPU. Let's find out if we have GPU access:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Project File Structure\n",
    "Our `docker-compose.yml` maps the local project root directory to `/home/devspace/project` in the container. Run the following cell for an overview of our project layout. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34m/home/devspace/project\u001b[0m\n",
      "├── \u001b[01;32mREADME.md\u001b[0m\n",
      "├── \u001b[01;34mdata\u001b[0m\n",
      "│   ├── \u001b[01;34mattack\u001b[0m\n",
      "│   ├── \u001b[01;34mattack_analyses_old\u001b[0m\n",
      "│   ├── \u001b[01;34mattack_analysis\u001b[0m\n",
      "│   ├── \u001b[01;34mexample_data\u001b[0m\n",
      "│   ├── \u001b[01;34mmodel\u001b[0m\n",
      "│   ├── \u001b[01;34mpreprocess\u001b[0m\n",
      "│   └── \u001b[01;34mquery_db\u001b[0m\n",
      "├── \u001b[01;34mdocker\u001b[0m\n",
      "│   ├── \u001b[01;34mapp\u001b[0m\n",
      "│   ├── \u001b[01;34mapp_dev\u001b[0m\n",
      "│   ├── \u001b[01;34mapp_resources\u001b[0m\n",
      "│   ├── \u001b[01;34mpg_init_scripts\u001b[0m\n",
      "│   └── \u001b[01;32mtuning_dbs.env\u001b[0m\n",
      "├── \u001b[01;34mdocs\u001b[0m\n",
      "│   ├── \u001b[01;32mdiscuscussion.md\u001b[0m\n",
      "│   └── \u001b[01;32mlong_desc_a.md\u001b[0m\n",
      "├── \u001b[01;34mlogs\u001b[0m\n",
      "│   └── test.log\n",
      "├── \u001b[01;34mnotebooks\u001b[0m\n",
      "│   ├── \u001b[01;34m__pycache__\u001b[0m\n",
      "│   ├── \u001b[01;32marchitecture_table.md\u001b[0m\n",
      "│   ├── \u001b[01;34mimages\u001b[0m\n",
      "│   ├── lstm_adversarial_attack.ipynb\n",
      "│   └── \u001b[01;32msrc_paths.py\u001b[0m\n",
      "└── \u001b[01;34msrc\u001b[0m\n",
      "    └── \u001b[01;34mlstm_adversarial_attack\u001b[0m\n",
      "\n",
      "20 directories, 8 files\n"
     ]
    }
   ],
   "source": [
    "!tree -L 2 /home/devspace/project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 `src/`\n",
    "The contents of `/home/devspace/project/src/lstm_adversarial_attack` are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34m/home/devspace/project/src/lstm_adversarial_attack\u001b[0m\n",
      "├── \u001b[01;34m__pycache__\u001b[0m\n",
      "├── \u001b[01;34mattack\u001b[0m\n",
      "├── \u001b[01;34mattack_analysis\u001b[0m\n",
      "├── \u001b[01;34mdataset\u001b[0m\n",
      "├── \u001b[01;34mmodel\u001b[0m\n",
      "├── \u001b[01;34mpreprocess\u001b[0m\n",
      "├── \u001b[01;34mquery_db\u001b[0m\n",
      "├── \u001b[01;34mtuning_db\u001b[0m\n",
      "└── \u001b[01;34mutils\u001b[0m\n",
      "\n",
      "9 directories\n"
     ]
    }
   ],
   "source": [
    "!tree -d -L 1 /home/devspace/project/src/lstm_adversarial_attack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Code in the sub-directories listed above forms our project pipeline: \n",
    " * **query_db** runs .sql queries to extract patient lab, vital sign, and in-hospital mortality data from the MIMIC-III PostgreSQL database.\n",
    " * **preprocess** transforms .sql query output into a form that can be input to PyTorch models. \n",
    " * **model** tunes and trains a PyTorch model for predicting in-hospital mortality based on lab and vital sign time-series data.\n",
    " * **attack** tunes and trains a PyTorch attack model that generates adversarial examples for the predictive model.\n",
    " * **attack_analysis** generates plots for visualizing characteristics of adversarial examples found by the attack model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 `data/`\n",
    "\n",
    "For each of the critical directories under `src/lstm_adversarial_attack/`, there is a corresponding directory under `data/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34m/home/devspace/project/data\u001b[0m\n",
      "├── \u001b[01;34mattack\u001b[0m\n",
      "├── \u001b[01;34mattack_analyses_old\u001b[0m\n",
      "├── \u001b[01;34mattack_analysis\u001b[0m\n",
      "├── \u001b[01;34mexample_data\u001b[0m\n",
      "├── \u001b[01;34mmodel\u001b[0m\n",
      "├── \u001b[01;34mpreprocess\u001b[0m\n",
      "└── \u001b[01;34mquery_db\u001b[0m\n",
      "\n",
      "7 directories\n"
     ]
    }
   ],
   "source": [
    "!tree -d -L 1 /home/devspace/project/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of the code we will be running is under `/home/devspace/project/src/lstm_adversarial_attack`, so let's change to that directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/devspace/project/src/lstm_adversarial_attack\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(\"/home/devspace/project/src/lstm_adversarial_attack\")\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Database Queries\n",
    "\n",
    "Raw ICU patient data can be extracted from the MIMIC-III database using modified versions of four `.sql`queries from the [MIT-LCP mimic-code repository](https://github.com/MIT-LCP/mimic-code/tree/main/mimic-iii/concepts/pivot). We connect to the database and execute the queries by running the [\\_\\_main__](../src/lstm_adversarial_attack/query_db/__main__.py) module of the [query_db](../src/lstm_adversarial_attack/query_db/\\_\\_init__.py) sub-package. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query 1 of 4\n",
      "Executing: /home/devspace/project/src/lstm_adversarial_attack/query_db/mimiciii_queries/icustay_detail.sql\n",
      "Done. Query time = 0.25 seconds\n",
      "Writing result to csv: /home/devspace/project/data/query_db/20240801173900712906/icustay_detail.csv\n",
      "Done. csv write time = 0.18 seconds\n",
      "\n",
      "Query 2 of 4\n",
      "Executing: /home/devspace/project/src/lstm_adversarial_attack/query_db/mimiciii_queries/pivoted_bg.sql\n",
      "Done. Query time = 8.32 seconds\n",
      "Writing result to csv: /home/devspace/project/data/query_db/20240801173900712906/pivoted_bg.csv\n",
      "Done. csv write time = 1.45 seconds\n",
      "\n",
      "Query 3 of 4\n",
      "Executing: /home/devspace/project/src/lstm_adversarial_attack/query_db/mimiciii_queries/pivoted_lab.sql\n",
      "Done. Query time = 10.44 seconds\n",
      "Writing result to csv: /home/devspace/project/data/query_db/20240801173900712906/pivoted_lab.csv\n",
      "Done. csv write time = 2.41 seconds\n",
      "\n",
      "Query 4 of 4\n",
      "Executing: /home/devspace/project/src/lstm_adversarial_attack/query_db/mimiciii_queries/pivoted_vital.sql\n",
      "Done. Query time = 36.11 seconds\n",
      "Writing result to csv: /home/devspace/project/data/query_db/20240801173900712906/pivoted_vital.csv\n",
      "Done. csv write time = 10.15 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python -m query_db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Implementation Details\n",
    "\n",
    "We will use the [`preprocess`](../src/lstm_adversarial_attack/preprocess/__init__.py) sub-package to transform information from the `.csv` files output by the `.sql` queries into numpy arrays (which can then be easily converted into PyTorch tensors). Running this sub-package instantiates a `Preprocessor` object with a `.preprocess_modules` attribute assigned by the following code in  [`preprocessor.py`](../src/lstm_adversarial_attack/preprocess/preprocessor.py):\n",
    "\n",
    "```\n",
    "self.preprocess_modules = [\n",
    "            prf.Prefilter(),\n",
    "            imc.ICUStayMeasurementCombiner(),\n",
    "            slb.FullAdmissionListBuilder(),\n",
    "            fb.FeatureBuilder(),\n",
    "            ff.FeatureFinalizer(),\n",
    "        ]\n",
    "```\n",
    "Each element of the `.preprocess_modules` attribute is a subclass of [`PreprocessModule`](../src/lstm_adversarial_attack/preprocess/preprocess_module.py).\n",
    "\n",
    "* [`Prefilter`](../src/lstm_adversarial_attack/preprocess/prefilter.py) reads the database query outputs into Pandas Dataframes, removes all data related to patients younger than 18 years in age, ensures consistent column naming formats, and takes care of datatype details.\n",
    "* [`ICUStayMeasurementCombiner`](../src/lstm_adversarial_attack/preprocess/icustay_measurement_combiner.py) performs various joins (aka \"merges\" in the language of Pandas) to combine lab and vital sign measurement data with ICU stay data.\n",
    "* [`FullAdmissionListBuilder`](../src/lstm_adversarial_attack/preprocess/sample_list_builder.py) generates a list consisting of one FullAdmissionData object per ICU stay. The attributes of a FullAdmissionData object include ICU stay info, and a dataframe containing the measurement and timestamp data for all vital sign and lab data associated with the ICU stay.\n",
    "* [`FeatureBuilder`](../src/lstm_adversarial_attack/preprocess/feature_builder.py) resamples the time series datafame to one-hour intervals, imputes missing data, winsorizes measurement values (with cutoffs at the 5th and 95th global percentiles), and normalizes the measuremnt values so all data are between 0 and 1.\n",
    "* [`FeatureFinalizer`](../src/lstm_adversarial_attack/preprocess/feature_finalizer.py) selects the data observation time window (default starts at hospital admission time and ends 48 hours after admission). This module outputs the entire dataset features as a list of numpy arrays, and the mortality labels as a list of integers. These data structures (saved as .pickle files) will be convenient starting points when the `tune_train` and `attack` sub-packages need to create PyTorch Datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Run the Preprocess Modules\n",
    "\n",
    "We run the preprocess code using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting preprocess session 20240801155105388032\n",
      "\n",
      "Running Prefilter\n",
      "Prefilter init time = 5.300392389297485\n",
      "Prefilter process time = 1.3465981483459473\n",
      "Prefilter export time = 0.23153233528137207\n",
      "Output saved in /home/devspace/project/data/preprocess/20240801155105388032/1_prefilter\n",
      "\n",
      "Running ICUStayMeasurementMerger\n",
      "ICUStayMeasurementMerger init time = 0.014288187026977539\n",
      "ICUStayMeasurementMerger process time = 11.847882747650146\n",
      "ICUStayMeasurementMerger export time = 1.049009084701538\n",
      "Output saved in /home/devspace/project/data/preprocess/20240801155105388032/2_merged_stay_measurements\n",
      "\n",
      "Running AdmissionListBuilder\n",
      "AdmissionListBuilder init time = 0.0033872127532958984\n",
      "AdmissionListBuilder process time = 17.79345965385437\n",
      "AdmissionListBuilder export time = 15.507328271865845\n",
      "Output saved in /home/devspace/project/data/preprocess/20240801155105388032/3_full_admission_list\n",
      "\n",
      "Running FeatureBuilder\n",
      "FeatureBuilder init time = 0.0023641586303710938\n",
      "Done building features for sample 5000/41960\n",
      "Done building features for sample 10000/41960\n",
      "Done building features for sample 15000/41960\n",
      "Done building features for sample 20000/41960\n",
      "Done building features for sample 25000/41960\n",
      "Done building features for sample 30000/41960\n",
      "Done building features for sample 35000/41960\n",
      "Done building features for sample 40000/41960\n",
      "FeatureBuilder process time = 119.93528175354004\n",
      "FeatureBuilder export time = 22.0468430519104\n",
      "Output saved in /home/devspace/project/data/preprocess/20240801155105388032/4_feature_builder\n",
      "\n",
      "Running FeatureFinalizer\n",
      "FeatureFinalizer init time = 0.00012373924255371094\n",
      "Received 41960 samples.\n",
      "Sending 37832 samples to final output.\n",
      "FeatureFinalizer process time = 11.119734287261963\n",
      "FeatureFinalizer export time = 1.212540864944458\n",
      "Output saved in /home/devspace/project/data/preprocess/20240801155105388032/5_feature_finalizer\n",
      "\n",
      "total time = 207.41151547431946\n"
     ]
    }
   ],
   "source": [
    "!python -m preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Overview of Feature Finalizer Output\n",
    "We can get information about the array shape and value distributions of the preprocessed data by running:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples = 37832\n",
      "\n",
      "Time Series Sequence Length Distribution\n",
      "----------------------------------------\n",
      "sequence_length     48\n",
      "count            37832\n",
      "\n",
      "Measurement Column Counts Distribution\n",
      "--------------------------------------\n",
      "num_measurements     19\n",
      "count             37832\n",
      "\n",
      "Min value of any element in any feature matrix = 0.0\n",
      "Max value of any element in any feature matrix = 1.0\n",
      "\n",
      "Class Labels Distribution\n",
      "-------------------------\n",
      "class_label      0     1\n",
      "count        33991  3841\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python preprocess/inspect_feature_finalizer_output.py 20240801155105388032\n",
    "# Change the command line argument to the preprocess ID of your actual preprocess session ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each sample in the FeatureFinalizer output is from a unique ICU stay, and consists of a 2D matrix of input features and a binary class label. Each column in a feature matrix corresponds to a particular lab or vital sign measurement, and each row in a feature matrix corresponds to the number of hours elapsed after a patient's hospital admission time. A class label of 1 indicates an in-hospital mortality event.\n",
    "\n",
    "When preprocessor parameters in `config.toml` are set to default values, the FeatureFinalizer output consists of 37832 samples, and the shape of all input feature arrays is 48 x 19, and approximately 11% of the preprocessed samples have class label = 1. Later, when we tune and train our predictive model, we will use oversampling techniques to deal with the significant class imbalance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 Performance\n",
    "\n",
    "On an Intel i7-13700K CPU, the above preprocessing work takes approximately 3.9 minutes. Achieving the same transformations on the same machine with preprocessing code from [[1](#References)] takes approximately 45 minutes. This time difference is largely due to the fact that the current project preprocess subpackage avoids using  unnecessary loops and relies heavily vectorized Pandas and Numpy operations.\n",
    "\n",
    "Additional time reduction could be achieved by parellelizing the preprocess computations with tools such as [pandaparallel](https://github.com/nalepae/pandarallel) or [pyspark](https://spark.apache.org/docs/3.3.1/api/python/index.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Architecture\n",
    "\n",
    "The starting point for our predictive model is based on the model in [1] and consists of the following layers:\n",
    "\n",
    "| Layer # | Description        | Input Shape                            | Parameters          | Output Shape           | Activation       |\n",
    "| ------- | ------------------ | -------------------------------------- | ------------------- | ---------------------- | ---------------- |\n",
    "| 1       | Bidirectional LSTM | (b, t<sub>max</sub> = 48, n<sub>meas</sub> = 19) | n<sub>LSTM</sub>    | (b, 2n<sub>LSTM</sub>) | a<sub>LSTM</sub> |\n",
    "| 2       | Dropoout           | (b, 2n<sub>LSTM</sub>)                 | P<sub>dropout</sub> | (b, 2n<sub>LSTM</sub>) | -                |\n",
    "| 3       | Fully Connected    | (b, 2n<sub>LSTM</sub>)                 | n<sub>FC</sub>      | (b, n<sub>FC</sub>)    | a<sub>FC</sub>   |\n",
    "| 4       | Output             | (b, n<sub>FC</sub>)                    | n<sub>out</sub> = 2 | (b, n<sub>out</sub>    | a<sub>out</sub>  |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameters from the above table are defined as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Parameter           | Description                                             |\n",
    "| ------------------- | ------------------------------------------------------- |\n",
    "| b                   | Batch size                                              |\n",
    "| t<sub>max</sub>     | Maximum input sequence length                           |\n",
    "| n<sub>meas</sub>    | Number of patient measurement types                     |\n",
    "| n<sub>LSTM</sub>    | Number of features in a LSTM hidden state               |\n",
    "| a<sub>LSTM</sub>    | Activation function for the LSTM output                 |\n",
    "| P<sub>dropout</sub> | Dropout probablity                                      |\n",
    "| n<sub>FC</sub>      | Numbef of nodes in the fully connected layer            |\n",
    "| a<sub>FC</sub>      | Activation function for the fully connected layer ouput |\n",
    "| n<sub>out</sub>     | Number of nodes in the output layer                     |\n",
    "| a<sub>out</sub>     | Activation function for the output layer                |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that n<sub>meas</sub>, n<sub>out</sub>, abd s<sub>max</sub> are fixed. We have chosen to always use all 19 patient measurement types, and our classification problem always has two classes. In our current data pipeline, data collected outside of a specified time window are removed during the final preprocessing phase. If we want the observation window to be tunable, it would be helpful to move the `preprocess.feature_finalizer` module into the `tune_attack` sub-package."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Hyperparameter Tuning\n",
    "\n",
    "### 9.1 Architectural hyperparameters\n",
    "\n",
    "The following table lists the ranges architectural parameters to be explored during hyperparameter tuning.\n",
    "\n",
    "| Parameter           | Tuning Type  | Values                            |\n",
    "| ------------------- | ------------ | --------------------------------- |\n",
    "| b                   | Discrete     | 2<sup>k</sup> , k = 5, 6, 7, 8    |                    \n",
    "| h<sub>LSTM</sub>    | Discrete     | 2<sup>k</sup> , k = 5, 6, 7       |\n",
    "| a<sub>LSTM</sub>    | Discrete     | ReLU, Tanh                        |\n",
    "| P<sub>dropout</sub> | Continuous   | 0.000 $\\textemdash$ 0.5000        |\n",
    "| h<sub>FC</sub>      | Discrete     | 2<sup>k</sup> , k = 4, 5, 6, 7, 8 |\n",
    "| a<sub>FC</sub>      | Discrete     | ReLU, Tanh                        |\n",
    "\n",
    "\n",
    "### 9.2 Trainer hyperparameters\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During hyperparameter tuning, we also explore different training optimization algorithms and learning rates.\n",
    "\n",
    "| Parameter     | Tuning Type | Values             |\n",
    "| ------------- | ----------- | ------------------ |\n",
    "| Optimizer     | Discrete    | SGD, RMSprop, Adam |\n",
    "| Learning Rate | Continuous  | 1e-5 - 1e-1        |\n",
    "\n",
    "When using the Adam optimizer, we always use the Pytorch default values of $\\beta_1 = 0.9, \\beta_2 = 0.999, \\epsilon = 10^{-8}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.3 Implementation Details\n",
    "The [`HyperParameterTuner`](../src/lstm_adversarial_attack/tune_train/hyperparameter_tuner.py) class in the [`tune_train`](../src/lstm_adversarial_attack/tune_train/__init__.py) sub-package implements a cross-validation tuning scheme that utilizes the [Optuna](https://optuna.org/) framework. The boundaries of hyperparameter space to explore during tuning are passed to the [`HyperParameterTuner`](../src/lstm_adversarial_attack/tune_train/hyperparameter_tuner.py) constructor in a [`X19MLSTMTuningRanges`](../src/lstm_adversarial_attack/tune_train/tuner_helpers.py) object. The default attribute of a [`X19MLSTMTuningRanges`](../src/lstm_adversarial_attack/tune_train/tuner_helpers.py) object are stored in the following config variables in [`config_settings`](../src/lstm_adversarial_attack/config_settings.py):\n",
    "```\n",
    "    TUNING_LOG_LSTM_HIDDEN_SIZE\n",
    "    TUNING_LSTM_ACT_OPTIONS\n",
    "    TUNING_DROPOUT\n",
    "    TUNING_LOG_FC_HIDDEN_SIZE\n",
    "    TUNING_FC_ACT_OPTIONS\n",
    "    TUNING_OPTIMIZER_OPTIONS\n",
    "    TUNING_LEARNING_RATE\n",
    "    TUNING_LOG_BATCH_SIZE\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A [`StratifiedKFold`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html) generator is used to assign samples to each fold. When selecting samples for each training batch, we use a [`DataLoader`](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) with a [`WeightedRandomSampler`](https://pytorch.org/docs/stable/data.html#torch.utils.data.WeightedRandomSampler) to oversample from the minority class (label = 1). For a given set of hyperparameters, the [`HyperParameterTuner.objective_fn`](../src/lstm_adversarial_attack/tune_train/hyperparaemter_tuner.py) method returns the mean validation loss across the K folds, and this mean loss is used as a minimization target by an Optuna [`TPESampler`](https://optuna.readthedocs.io/en/stable/reference/samplers/generated/optuna.samplers.TPESampler.html) to select new sets of hyperparameters for additional trials. [`HyperParameterTuner`](../src/lstm_adversarial_attack/tune_train/hyperparaemter_tuner.py) also uses an Optuna [`MedianPruner`](https://optuna.readthedocs.io/en/stable/reference/generated/optuna.pruners.MedianPruner.html) to stop unpromising trials early.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.4 Starting a New Hyperparameter Tuning Study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before starting, a few things to note:\n",
    "* Depending your GPU compute power, running the full 30 trials could take 2 - 20 hours.\n",
    "* Results will be saved to a newly created directory (with a timestamp-based name) under `data/tune_train/hyperparameter_tuning`. \n",
    "* If the study is stopped early (via CTRL-C or the Jupyter Stop button), learning from whatever trials have completed up to that point will be saved.\n",
    "* While the tuning trials are running, look ahead to the next Markdown cell for instructions on how to monitor progress in Tensorboard (depending on your notebook output settings you may need to scroll down to see that cell)\n",
    "\n",
    "We can start a new hyperparaemter tuning study using the [`tune_new`](../src/lstm_adversarial_attack/tune_train/tune_new.py) module from the [`tune_train`](../src/lstm_adversarial_attack/tune_train/__init__.py) subpackage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from lstm_adversarial_attack.tune_train import tune_new\n",
    "my_completed_study = tune_new.main(num_trials=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python model/tune_new.py --num_trials 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.5 Monitor Tuning Progress with Tensorboard\n",
    "\n",
    "While we are tuning hyperparameters, we can monitor results in Tensorboard. One (relatively straightforward) way to start Tensorboard is to first launch a `zsh` shell inside the project container:\n",
    "```\n",
    "$ docker exec -it lstm_aa_app_dev /bin/zsh\n",
    "```\n",
    "Next, at the container `zsh` prompt, run the following command to start a Tensorboard server:\n",
    "\n",
    "```\n",
    "> tensorboard --logdir=/home/devspace/project/data/hyperparameter_tuning/continued_trials/tensorboard --host=0.0.0.0\n",
    "```\n",
    "Then, in your browser, go to: `http://localhost:6006/` You should see something like the screenshot below.  The x-axis for all plots is epoch number. (Unfortunately, there is no good way to add axis labels in Tensorboard.) In this example we are in the middle of running trial #21. Trial #20 completed the default number of epochs per fold (100). Trial #19 only ran 20 epochs because it was pruned by the Optuna `MeadianPruner`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![tensorboard_image](images/tensorboard_hyperparameter_tuning.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.6 Run Additional Trials on an Exiting Tuning Study\n",
    "\n",
    "If we want to run additional trials using the results saved from a tuning study that previously ran, we can use the [`tune_train.tune_resume`](../src/lstm_adversarial_attack/tune_train/tune_resume.py) module.  When we resume an existing study, the Optuna framework can use learning from earlier trials in the study to choose conditios for the new trials. The new trial results are saved to the same directory and `optuna.Study` filepath containing results from the study's previous trials.\n",
    "\n",
    "We use the next code cell to resume tuning with an existing Study. When we do not provide an argument for tune_resume.main study_dir parameter (as is the case below), we default to the directory under `data/tune_train/hyperparameter_tuning` that contains the most recently modified `optuna_study.pickle` file.\n",
    "\n",
    ">**Note** If we want to use a study other than the most recently modified one, our call to `tune_resume.main` would look something like this:\n",
    "\n",
    ">`tune_resume.main(study_dir=/home/devspace/project/data/tune_train/hyperparameter_tuning/2023-07-27_17_04_20.069961/checkpoints_tuner,\n",
    "> num_trials=30)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from lstm_adversarial_attack.tune_train import tune_resume\n",
    "tune_resume.main(num_trials=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python tune_train/tune_resume.py --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_set.TUNER_NUM_TRIALS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python model/tune_resume.py --num_trials 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.7 Select Final Hyperparameters\n",
    "When we are done tuning, we can view our best set of hyperparameters by examining the `Optuna.Study` object from our above tuning run(s).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lstm_adversarial_attack.resource_io as rio\n",
    "\n",
    "study_path = Path(\"/home/devspace/project/data/tune_train/hyperparameter_tuning/continued_trials/checkpoints_tuner/optuna_study.pickle\")\n",
    "\n",
    "study = rio.ResourceImporter().import_pickle_to_object(\n",
    "    path=study_path\n",
    ")\n",
    "\n",
    "print(f\"The best trial result is from trial # {study.best_trial.number}.\\n\")\n",
    "print(\"The set of hyperparameters from this trial are:\")\n",
    "pprint.pprint(study.best_params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.8 Run K-Fold Cross Validation with \"Best\" Hyperparameters and Extended Training (More Epochs)\n",
    "In the above tuning runs, we only run 100 epochs per fold (in the interest of reducing compute requirements). Based on the validation loss and AUC curves, it appears that we could improve our predictive performance (i.e. decrease validation loss, and increase AUC) by training longer. We now run another round of Stratified K-fold cross-validation with our best set of parameters with a larger number of epochs.\n",
    "\n",
    "#### 9.8.1 Notes on our Method\n",
    "Some caveats about our methodology:\n",
    "* We are using \"flat\" cross-validation (as was done in previous studies on this dataset). This method computationally less expensive than nested cross-validation. Flat cross-validation has the potential to overestimate of model performance. In many cases the magnitude of overestimation is small. We also mitigate this effect by using a different set of (randomly generated) fold assignments than was used for hyperparameter tuning. \n",
    "* By selecting our hyperparameters based on the smaller number of epochs (100), we favor models that are faster to to train. It is possible that using a larger number of epochs in the tuning runs would have yielded a different (and better) set of \"best\" hyperparameters, but would also be computationally more expensive.\n",
    "\n",
    "\n",
    "#### 9.8.2 Instantiate a CrossValidatorDriver\n",
    "We use a CrossValidatorDriver object to run cross-validation with a single set of hyperparameters: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lstm_adversarial_attack.tune_train.cross_validator_driver as cvd\n",
    "import lstm_adversarial_attack.x19_mort_general_dataset as xmd\n",
    "\n",
    "cv_driver = cvd.CrossValidatorDriver.from_study_path(\n",
    "        device=cur_device,\n",
    "        dataset=dataset,\n",
    "        study_path=cfg_paths.ONGOING_TUNING_STUDY_PICKLE\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets look at the data members of `cv_driver`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint.pprint(cv_driver.__dict__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will run 5-fold cross-validation using 1000 epochs per fold. We will evaluate performance and save a checkpoint once every 10 epochs. These settings are determined by the values of `CV_DRIVER_EPOCHS_PER_FOLD`, `CV_DRIVER_NUM_FOLDS`, `CV_DRIVER_EVAL_INTERVAL`, and `CV_DRIVER_EVALS_PER_CHECKPOINT` in `lstm_adversarial_attacker.config_settings`. The `.from_study_path()` class method we used to construct `cv_driver` extracts the best set of hyperparameters from `study_path` and passes them to the CrossValidationDriver constructor.\n",
    "\n",
    "#### 9.8.3 Run Cross-Validation\n",
    "We now call `cv_driver`'s `.run()` method to start the cross-validation runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "collapsed"
    ]
   },
   "outputs": [],
   "source": [
    "cv_driver.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python model/train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9.8.4 Monitor Cross-Validation Progress in Tensorbard\n",
    "Near the start of the terminal output from the previous code cell, look for the lines:\n",
    "```\n",
    "Checkpoints will be saved in:\n",
    "/home/devspace/project/data/cv_assessments/<timestamped_directory_name>/tensorboard\n",
    "```\n",
    "Then, start a zsh shell inside the app container, and launch tensorboard server:\n",
    "```\n",
    "$ docker exec -it lstm_aa_app /bin/zsh\n",
    "$ tensorboard --logdir=/home/devspace/project/data/cv_assessments/<timestamped_directory_name>/tensorboard --host=0.0.0.0\n",
    "```\n",
    "The Tensorboard output can now be viewed in your browswer at http://localhost:6006\n",
    "\n",
    "This Tensorboard screenshot was taken at the end of a 5-fold, 1000 epoch per fold cross-validation run.\n",
    "![tensorboard_image](images/tensorboard_5fold_cv_best_params_1000epochs.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9.8.5 Why Do We See Continued (Slow) Increase in Predictive Performance Up To Such High (1000) Epoch Counts?\n",
    "\n",
    "The above AUC and validation loss curves show continued (though diminishing) improvement in predictive performance during the entire 1000 epochs. The fact that we do not observe any sign of overfitting at such a large number of epochs is somewhat unusual. A likely cause of this behavior is the `WeightedRandomSampler` used in our training `DataLoaders`. Samples with our minority class label (`mortality = 1`) only represent ~15% of the total dataset. To deal with this imbalanced dataset, we oversample from the minority class and undersample from the majority class when creating batches of samples for training. In our current implementation, some samples from the majority class go unseen by the `StandardModelTrainer` for a large number of epochs. The number of unseen samples slowly dwindles (and the amount of information available for training slowly increases), even at very high epoch counts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9.8.6 Summarize Results\n",
    "We can use a CrossValidationSummarizer to identify and summarize each fold's best-performing checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lstm_adversarial_attack.model.cross_validation_summarizer as cvs\n",
    "cv_summarizer = cvs.CrossValidationSummarizer.from_cv_checkpoints_dir()\n",
    "optimal_results_df = cv_summarizer.get_optimal_results_df(\n",
    "        metric=cvs.EvalMetric.VALIDATION_LOSS,\n",
    "        optimize_direction=cvs.OptimizeDirection.MIN,\n",
    "    )\n",
    "\n",
    "optimal_results_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get the mean and standard deviation of each performance metric using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_results_df.describe().loc[[\"mean\", \"std\"], (optimal_results_df.columns != \"epoch\") & (optimal_results_df.columns != \"fold\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.9 Comparison with Prior Work\n",
    "\n",
    "The table below compares the predictive performance of the LSTM model in this work with other LSTM-based models using the same dataset. The current model shows the best predictive performance among all models in the table based on AUC and F1 scores. \n",
    "\n",
    "\n",
    "|  | Authors       | Model      | Input Features | AUC             | F1              | Precision       | Recall          |\n",
    "|-|------------|------------|----------------|-----------------|-----------------|-----------------|-----------------|\n",
    "|1 |Sun et al.  | LSTM-128 + FC-32 + FC-2 | [13 labs, 6 vitals] x 48 hr  | 0.9094 (0.0053) | 0.5429 (0.0194) | 0.4100 (0.0272) | 0.8071 (0.0269) |\n",
    "|2 |Tang et al. | LSTM-256 + FC-2 | [13 labs, 6 vitals] x 48 hr + demographic data  | 0.949 (0.003) | 0.623 (0.012) | \n",
    "| 3|Tang et al. | CNN + LSTM-256 + FC-2 | [13 labs, 6 vitals] x 48 hr + demographic data | 0.940 (0.0071) | 0.633 (0.031) | \n",
    "|4 |Tang et al. | CNN + LSTM-256 + FC-2 | [13 labs, 6 vitals] x 48 hr | 0.933 (0.006) | 0.587 (0.025) |\n",
    "|5 |Tang et al. | LSTM-256 + FC-2 | [13 labs, 6 vitals] x 48 hr | 0.907 (0.006) | 0.526 (0.013) |\n",
    "|6 |This work   | LSTM-128 + FC-16 + FC-2 | [13 labs, 6 vitals] x 48 hr  | 0.9657 (0.0035) | 0.9669 (0.0038) | 0.9888 (0.0009) | 0.9459 (0.0072) |\n",
    "\n",
    "> **Notes** LSTM-X indicates an LSTM with X hidden layers. FC-X indicates a fully connected layer with an output size of X. All LSTMs are bidirectional. The demographic data used in studies #2 and #3 was obtained from MIMIC-III.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Run Adversarial Attack Algorithm on the Trained Model\n",
    "\n",
    "### 10.1 Adversarial Loss and Regularization\n",
    "Our method of adversarial attack is similar to Chen et al.'s approach that uses an adversarial loss function and L1 regularization. When attacking a binary classification model with trained parameters $\\theta$, we start with the input feature matrix $X$ of a sample that the model correctly predicts to be in class $t_{c}$, so  $M(X) = t_{c}$ where $M$ is the model's prediction function. We then search for a perturbation matrix $P$ that meets the condition:\n",
    "$$\n",
    "M(X + P) \\ne t_{c}\n",
    "$$\n",
    "Since we are dealing with binary classification, this condition is equivalent to:\n",
    "$$\n",
    "M(X + P) = \\neg{t_{c}}\n",
    "$$\n",
    "where $\\neg{t_c}$ is the negation of $t_c$. Defining a perturbed feature matrix $\\widetilde{X} = X + P$ , an adversarial loss function can be written as:\n",
    "$$\n",
    "max\\{[Logit(\\widetilde{X})]_{t_c} - [Logit(\\widetilde{X})]_{\\neg{t_c}}, - \\kappa \\}\n",
    "$$\n",
    "\n",
    "When running perturbed input $\\widetilde{X}$ through a forward pass, $[Logit(\\widetilde{X})]_{t_c}$ and $[Logit(\\widetilde{X})]_{\\neg{t_c}}$ are the **pre-activation** values at the nodes corresponding to $t_c$ and $\\neg{t_c}$ the in 2-node final layer. A value $\\ge 0$ is chosen for $\\kappa$. Using a small non-zero value of $\\kappa$ will prevent an attack algorithm from optimizing toward an infinitesimally small gap between $[Logit(\\widetilde{X})]_{t_c}$ and $[Logit(\\widetilde{X})]_{\\neg{t_c}}$ while still targeting the small difference we want for an adversarial example.\n",
    "\n",
    "To encourage an attack algorithm to find sparse perturbations, the following regularized version of Equation  () is used  \n",
    "\n",
    "$$\n",
    "max\\{[Logit(\\widetilde{X})]_{y_\\theta} - [Logit(X)]_{\\widetilde{y}_\\theta}, - \\kappa \\} + \\lambda||\\widetilde{X}-X||_1\n",
    "$$\n",
    "\n",
    "where $\\lambda$ is the L1 regularization constant. Equation () can be minimized by subgradient descent or by an Iterative Soft-Thresholding Algorithm (ISTA). The latter approach typically converges faster. \n",
    "\n",
    "\n",
    "### 10.2 Attack Algorithm and Regularization\n",
    "\n",
    "Adversarial attacks on a particular model and dataset input features are managed by an `AdversarialAttackTrainer`. In the procedure outlined below, we discover an adversarial example any time we find $[Logit(\\widetilde{X})]_{\\neg{t_c}} > [Logit(\\widetilde{X})]_{t_c}$, even if we have not converged near a minimum value of Equation (). We attack each batch of samples for a fixed number of iterations, regardless of how many (if any) adversarial examples are found.\n",
    "\n",
    "1. A `LogitNoDropoutModelBuiler` creates a modified version of the target model. The modified model has all dropout probabilities set to zero, and does not have an activation function on the output layer.\n",
    "2. Batches of input features are run through a `FeaturePerturber` (implemented in `attack.feature_perturber`) that generates slightly modified versions of original features\n",
    "3. The perturbed features are run through the modified model that was built by the `LogitNoDropoutModelBuiler` to obtain values for $[Logit(\\widetilde{X})]_{t_c}$ and $[Logit(\\widetilde{X})]_{\\neg{t_c}}$\n",
    "4. An instance of custom PyTorch loss function ` AdversarialLoss`, which implements Equation (), calculates a loss tensor\n",
    "5. The Pytorch `.backward()`  method of the loss tensor finds the gradient of the loss with respect to the elements of the `FeaturePerturber.perturbation` tensor\n",
    "\n",
    "6. If the current $Logit$ values resulting from a sample's perturbed input features represent an adversarial example, and the example is either the first or lowest loss example for that sample, the perturbations and other details are stored in a `BatchResult` object.\n",
    "\n",
    "7. A Pytorch optimizer uses the loss gradient to calculate and apply adjustments to the perturbations\n",
    "\n",
    "8. The `AdversarialAttackTrainer.apply_soft_bounded_threshold()` method performs ISTA thresholding on the perturbations\n",
    "\n",
    "9. The perturbations (which have been adjusted by the optimizer *and* ISTA thresholding, are used in step 1 of the next attack iteration.\n",
    "\n",
    "Two key points from above procedure are: (1) Unlike the method used in [], we do not stop attacking an example upon finding a single adversarial perturbation for it.  (2) We use a combination of subgradient descent (in step 7), and ISTA (in step 8) to minimize (or at least reduce) the value of equation (). We do not know if this approach is guaranteed to converge to a minimum in the adversarial loss function, but empirically, we find this subgradient descent + ISTA more effective at finding sparse adversarial examples than either method is on its own.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.3 Attack Hyperparameter Tuning\n",
    "\n",
    "Before running an attack on the entire dataset, we tune attack hyperparameters with help from `optuna`. Our approach here is not as rigourous as the one we used for predictive model tuning. We just use small fraction of the total dataset for tuning, and no cross-validation is involved.\n",
    "\n",
    "#### 10.3.1 Viewing / Setting the Tuning Ranges\n",
    "\n",
    "First, let's look at the current values of the project config variables that determine how an attack hyperparameter tuning session will run.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Kappa: min = {cfg_set.ATTACK_TUNING_KAPPA[0]}, max = {cfg_set.ATTACK_TUNING_KAPPA[1]}\")\n",
    "print(f\"Lambda: min = {cfg_set.ATTACK_TUNING_LAMBDA_1[0]}, max = {cfg_set.ATTACK_TUNING_LAMBDA_1[1]}\")\n",
    "print(f\"Optimizer: {cfg_set.ATTACK_TUNING_OPTIMIZER_OPTIONS}\")\n",
    "print(f\"Learning rate: min = {cfg_set.ATTACK_TUNING_LEARNING_RATE[0]}, max = {cfg_set.ATTACK_TUNING_LEARNING_RATE[1]}\")\n",
    "print(f\"Batch size: min = {2 ** cfg_set.ATTACK_TUNING_LOG_BATCH_SIZE[0]}, max = {2 ** cfg_set.ATTACK_TUNING_LOG_BATCH_SIZE[1]}\")\n",
    "print(f\"Attack iterations per batch: {cfg_set.ATTACK_TUNING_EPOCHS}\")\n",
    "print(f\"Max number of samples: {cfg_set.ATTACK_TUNING_MAX_NUM_SAMPLES}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These values are stored as variables in `src/lstm_adversarial_attack/config_settings.py` and can be modified as needed to customize a tuning session.\n",
    "\n",
    ">**Note** The `ATTACK_TUNING_MAX_NUM_SAMPLES` specifies the number of samples to be considered for attack. However, samples that are misclassified by the target model are not attacked, so the actual number of samples used for tuning will be slightly lower."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10.3.2 Running an AttackHyperParameterTuner\n",
    "\n",
    "We can run a new attack hyperparameter tunin session with function `initiate_attack_tuning_study()` from the `attack.tune_attacks` module. The `target_mode_assessment_type` determines whether we use a model trained by cross-validation or single-fold training. The default behavior is to choose the most recent result of whichever assessment type is specified. We can influence the type of adversarial perturbation our tuned algorithm will produce through the argument passed to the `objective` parameter. We use the return value of any method of class `AttackTunerObjectivesBuilder`. Current options are:\n",
    "\n",
    "| Objective                                                   | Maximizes                                                    |\n",
    "| ----------------------------------------------------------- | ------------------------------------------------------------ |\n",
    "| `sparsity()`        | Sum of the perturbation sparsities of the lowest loss adversarial example of each sample |\n",
    "| `max_num_nonzero_perts()` | Number of adversarial perturbations with only one non-zero element |\n",
    "| `sparse_small()`           | Sum of (sparsity / L1 norm) of the lowest loss adversarial example of each sample |\n",
    "| `sparse_small_max()`       | Sum of (sparsity / largest magnitude of any perturbation element) of lowest loss adversarial example of each sample |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lstm_adversarial_attack.attack.attack_hyperparameter_tuner as aht\n",
    "import lstm_adversarial_attack.attack.tune_attacks as tua\n",
    "\n",
    "# initial_attack_tuning_study = tua.main()\n",
    "help(tua.main)\n",
    "\n",
    "# initial_attack_tuning_study = tua.start_new_tuning(\n",
    "#         num_trials=50,\n",
    "#         # target_model_assessment_type=amr.ModelAssessmentType.KFOLD,\n",
    "#         objective=aht.AttackTunerObjectivesBuilder.sparse_small_max(),\n",
    "#     )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results will be saved in a subdirectory of `/home/devspace/project/data/attack/attack_hyperparameter_tuning`. If we want to run additional trials for an existing study, we can use the following code. Again, the default behavior is to use the newest existing study."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "continued_study = tua.resume_tuning(num_trials=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10.3.3 Attacking the full dataset\n",
    "\n",
    "Next we use function  `attack_with_tuned_params()` to attack all correctly classified samples using the results of our latest hyperparamter tuning session. Results will be saved in a subdirectory of /home/devspace/project/data/attack/frozen_hyperparameter_attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lstm_adversarial_attack.attack.attack as atk\n",
    "atk.attack_with_tuned_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Attack Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lstm_adversarial_attack.attack_analysis.attack_analysis as ata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lstm_adversarial_attack.attack_analysis import attack_analysis_driver as aad\n",
    "aad.plot_latest_result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "<a id=\"ref_01\"></a>1. [Sun, M., Tang, F., Yi, J., Wang, F. and Zhou, J., 2018, July. Identify susceptible locations in medical records via adversarial attacks on deep predictive models. In *Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery & data mining* (pp. 793-801).](https://dl.acm.org/doi/10.1145/3219819.3219909)\n",
    "\n",
    "<a id=\"ref_02\">2.</a> [Tang, F., Xiao, C., Wang, F. and Zhou, J., 2018. Predictive modeling in urgent care: a comparative study of machine learning approaches. *Jamia Open*, *1*(1), pp.87-98.](https://academic.oup.com/jamiaopen/article/1/1/87/5032901)\n",
    "\n",
    "<a><a id=\"ref_03\">3.</a> </a>[Johnson, A., Pollard, T., and Mark, R. (2016) 'MIMIC-III Clinical Database' (version 1.4), *PhysioNet*.](https://doi.org/10.13026/C2XW26) \n",
    "\n",
    "<a id=\"ref_04\">4.</a> [Johnson, A. E. W., Pollard, T. J., Shen, L., Lehman, L. H., Feng, M., Ghassemi, M., Moody, B., Szolovits, P., Celi, L. A., & Mark, R. G. (2016). MIMIC-III, a freely accessible critical care database. Scientific Data, 3, 160035.](https://www.nature.com/articles/sdata201635)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
